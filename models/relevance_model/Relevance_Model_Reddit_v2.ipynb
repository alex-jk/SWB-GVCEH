{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu3Npnfc5Vy24Viso3twzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/SWB-GVCEH/blob/main/models/relevance_model/Relevance_Model_Reddit_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dKNqfGbtPPjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7b95bc-ea85-45dc-a218-a1dd28a040b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setfit\n",
        "!pip install tqdm\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9GjEawelbPR",
        "outputId": "ee9db2d3-efbd-4e13-8e9d-4a896d618fe1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setfit in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: datasets>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from setfit) (2.15.0)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from setfit) (2.2.2)\n",
            "Requirement already satisfied: evaluate>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from setfit) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from setfit) (0.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from setfit) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate>=0.3.0->setfit) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->setfit) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->setfit) (4.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (0.16.0+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (0.1.99)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->setfit) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->setfit) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.1->setfit) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2023.3.post1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.1->setfit) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.3.0->setfit) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from setfit import SetFitModel\n",
        "from tqdm.notebook import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model_sent_transformer = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "RNoP4TmNQHwH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Import reddit datasets"
      ],
      "metadata": {
        "id": "SobUCnlncR3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/SWB-GVCEH/VictoriaBC_data_updated.csv'\n",
        "VictoriaBC_data_nodups = pd.read_csv(file_path)\n",
        "VictoriaBC_data_nodups = VictoriaBC_data_nodups.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(\"\\nVictoria BC data ----------------\")\n",
        "print(VictoriaBC_data_nodups.shape)\n",
        "print(VictoriaBC_data_nodups.columns)\n",
        "print(VictoriaBC_data_nodups.head())\n",
        "\n",
        "Sooke_data_nodups = pd.read_csv('/content/drive/My Drive/SWB-GVCEH/Sooke_data.csv')\n",
        "Sooke_data_nodups = Sooke_data_nodups.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSooke data ----------------\")\n",
        "print(Sooke_data_nodups.shape)\n",
        "print(Sooke_data_nodups.columns)\n",
        "print(Sooke_data_nodups.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65iKSFNvPp7b",
        "outputId": "a06db92f-2d2a-479c-8145-61609c41650d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Victoria BC data ----------------\n",
            "(8998, 6)\n",
            "Index(['Subreddit', 'Title', 'Text', 'Submission ID', 'User ID',\n",
            "       'Search Term'],\n",
            "      dtype='object')\n",
            "    Subreddit                                              Title  \\\n",
            "0  VictoriaBC  True change around homelessness from the homel...   \n",
            "1  VictoriaBC  New transitional housing facility on Douglas o...   \n",
            "2  VictoriaBC   Where to buy a reasonably priced Christmas tree?   \n",
            "3  VictoriaBC  Looking for a Christmas tree? Please consider ...   \n",
            "4  VictoriaBC                              Free clothing places?   \n",
            "\n",
            "                                                Text Submission ID  \\\n",
            "0  I've started my own organization to expose the...        zen3ao   \n",
            "1                                                NaN        5yah6z   \n",
            "2  Went to the Christmas tree farm today and thou...       17yjdiw   \n",
            "3                                                NaN        7h0e58   \n",
            "4  Hey so long story short I moved here from Sask...       169ywlj   \n",
            "\n",
            "          User ID              Search Term  \n",
            "0   Ill_Kale_4507            #PovertyPimps  \n",
            "1    rednightmare            #PovertyPimps  \n",
            "2       Doomlemur  1upparents anawim house  \n",
            "3  Necrostopheles  1upparents anawim house  \n",
            "4   playertwolol3  1upparents anawim house  \n",
            "\n",
            "Sooke data ----------------\n",
            "(105, 6)\n",
            "Index(['Subreddit', 'Title', 'Text', 'Submission ID', 'User ID',\n",
            "       'Search Term'],\n",
            "      dtype='object')\n",
            "  Subreddit                                              Title  \\\n",
            "0     Sooke  Design Your Own Journal - Vancouver Island Reg...   \n",
            "1     Sooke  Are there sexual predator watchdog groups in S...   \n",
            "2     Sooke                                       Commute info   \n",
            "3     Sooke                                   Camping in Sooke   \n",
            "4     Sooke                               addy Coming to Sooke   \n",
            "\n",
            "                                                Text Submission ID  \\\n",
            "0  Design your own journal for yourself or a jour...        t61vrr   \n",
            "1  My girlfriend was recently traveling on Vancou...        p00u2m   \n",
            "2  Hey guys, not sure how active this sub is, but...        4ti4yn   \n",
            "3  Hi, weâ€™re planning on camping in Sooke on Apri...        m59ist   \n",
            "4  Hi All - addy ( r/addyinvest ) is an online in...        rprnmu   \n",
            "\n",
            "         User ID                            Search Term  \n",
            "0        VIRL_CC  victoriavisitor aids vancouver island  \n",
            "1   scoobysmokes  victoriavisitor aids vancouver island  \n",
            "2  Slappaadabass  victoriavisitor aids vancouver island  \n",
            "3   VanillaWrong                                Camping  \n",
            "4  stephenjagger         the victoria real estate board  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = 'https://drive.google.com/uc?id=1ANE3_UkNi2UGyQpHr8Ujz1QwBNKxmcTl'\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "additional_reddit_df = pd.read_csv(file_url)\n",
        "additional_reddit_df = additional_reddit_df.drop_duplicates().reset_index(drop=True)\n",
        "additional_reddit_df.rename(columns={'Body': 'Text'}, inplace=True)\n",
        "\n",
        "print(\"\\nAdditional reddit data ----------------\")\n",
        "print(additional_reddit_df.shape)\n",
        "print(additional_reddit_df.columns)\n",
        "print(additional_reddit_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkBXoorBayFL",
        "outputId": "f4866a41-17b0-44a1-a991-7720e38c2a98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Additional reddit data ----------------\n",
            "(2057, 4)\n",
            "Index(['Subreddit', 'Title', 'Text', 'Comments'], dtype='object')\n",
            "               Subreddit                                              Title  \\\n",
            "0  OakBayBritishColumbia                            Oak Bay high right now.   \n",
            "1  OakBayBritishColumbia  Food share for Ukrainian refugees on Vancouver...   \n",
            "2  OakBayBritishColumbia     Lost Budgie - Cadboro Bay (Willows Elementary)   \n",
            "3  OakBayBritishColumbia  What are 3 things I must see when visiting Oak...   \n",
            "4  OakBayBritishColumbia  Yes, this is a leaf blower rant, but hear me o...   \n",
            "\n",
            "                                                Text  \\\n",
            "0  Anyone know what the hell is going on at Oak B...   \n",
            "1                                                NaN   \n",
            "2  My friends blue budgie flew out of the house l...   \n",
            "3                                            Thanks!   \n",
            "4  I have a neighbour who uses his leaf blower to...   \n",
            "\n",
            "                                            Comments  \n",
            "0  ['https://www.cheknews.ca/hold-and-secure-orde...  \n",
            "1                                                 []  \n",
            "2                                                 []  \n",
            "3  [\"Here are four things to see in each of the m...  \n",
            "4                                                 []  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Combine all data into one df"
      ],
      "metadata": {
        "id": "T7tR-mffcNSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the DataFrames\n",
        "select_cols = ['Subreddit', 'Title', 'Text']\n",
        "combined_df = pd.concat([VictoriaBC_data_nodups[select_cols], Sooke_data_nodups[select_cols], additional_reddit_df[select_cols]], ignore_index=True)\n",
        "\n",
        "# Resetting the index\n",
        "combined_df.reset_index(drop=True, inplace=True)\n",
        "print(\"\\Combined data ----------------\")\n",
        "print(combined_df.shape)\n",
        "print(combined_df.columns)\n",
        "print(combined_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1e2KdfycAbz",
        "outputId": "7c6930e6-0353-48cc-e53b-27c4416c5d0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Combined data ----------------\n",
            "(11160, 3)\n",
            "Index(['Subreddit', 'Title', 'Text'], dtype='object')\n",
            "    Subreddit                                              Title  \\\n",
            "0  VictoriaBC  True change around homelessness from the homel...   \n",
            "1  VictoriaBC  New transitional housing facility on Douglas o...   \n",
            "2  VictoriaBC   Where to buy a reasonably priced Christmas tree?   \n",
            "3  VictoriaBC  Looking for a Christmas tree? Please consider ...   \n",
            "4  VictoriaBC                              Free clothing places?   \n",
            "\n",
            "                                                Text  \n",
            "0  I've started my own organization to expose the...  \n",
            "1                                                NaN  \n",
            "2  Went to the Christmas tree farm today and thou...  \n",
            "3                                                NaN  \n",
            "4  Hey so long story short I moved here from Sask...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:7: DeprecationWarning: invalid escape sequence '\\C'\n",
            "<>:7: DeprecationWarning: invalid escape sequence '\\C'\n",
            "<ipython-input-16-f6c7613a94f5>:7: DeprecationWarning: invalid escape sequence '\\C'\n",
            "  print(\"\\Combined data ----------------\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load twitter data"
      ],
      "metadata": {
        "id": "CD9thJMwieq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file names\n",
        "file_names = [\n",
        "    'GVCEH-tweets-combined_2023-02-08.csv',\n",
        "    'GVCEH-tweets-combined_2023-01-30.csv',\n",
        "    'GVCEH-tweets-combined_2023-01-21.csv',\n",
        "    'GVCEH-tweets-combined_2023-01-12.csv'\n",
        "]\n",
        "\n",
        "# Base URL for raw files in the GitHub repository\n",
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/data/processed/twitter/github_actions/'\n",
        "\n",
        "# Initialize a list to collect the DataFrames\n",
        "dfs = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    # Construct the full URL for the current file\n",
        "    file_url = base_url + file_name\n",
        "    # Read the CSV file\n",
        "    current_df = pd.read_csv(file_url)\n",
        "    # Append the DataFrame to the list\n",
        "    dfs.append(current_df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "tweets_combined_df = pd.concat(dfs, ignore_index=True)\n",
        "# Remove duplicates\n",
        "tweets_combined_df = tweets_combined_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Displaying the first few rows of the DataFrame\n",
        "print(tweets_combined_df.shape)\n",
        "print(tweets_combined_df.columns)\n",
        "print(tweets_combined_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kO4UjOdhuuO",
        "outputId": "9273cf9e-5cbc-41aa-897a-6f49b97e8196"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5435, 17)\n",
            "Index(['Unnamed: 0', 'text', 'scrape_time', 'tweet_id', 'created_at',\n",
            "       'reply_count', 'quote_count', 'like_count', 'retweet_count',\n",
            "       'geo_full_name', 'geo_id', 'username', 'num_followers',\n",
            "       'search_keywords', 'search_neighbourhood', 'sentiment', 'score'],\n",
            "      dtype='object')\n",
            "   Unnamed: 0                                               text  \\\n",
            "0           0  RT pressjournal: Colonsay islanders and people...   \n",
            "1           1  Colonsay islanders and people who have left th...   \n",
            "2           7  @ArianeBurgessHI Serviced plots for 25k are ex...   \n",
            "3           9  RT @VicBuilders: \"25-unit townhome development...   \n",
            "4          27  @OurNewHomecoach @laughatthemoon2 There is so ...   \n",
            "\n",
            "                  scrape_time             tweet_id                 created_at  \\\n",
            "0  2023-02-07 03:20:43.040309  1622564995115503616  2023-02-06 11:56:55+00:00   \n",
            "1  2023-02-07 03:20:43.040317  1622550741599625221  2023-02-06 11:00:16+00:00   \n",
            "2  2023-02-07 03:20:51.207543  1622549961081487360  2023-02-06 10:57:10+00:00   \n",
            "3  2023-02-07 03:20:56.511307  1622671505778778152  2023-02-06 19:00:09+00:00   \n",
            "4  2023-02-07 03:21:18.339644  1622767215811641344  2023-02-07 01:20:28+00:00   \n",
            "\n",
            "   reply_count  quote_count  like_count  retweet_count geo_full_name geo_id  \\\n",
            "0          0.0          0.0         0.0            0.0           NaN    NaN   \n",
            "1          0.0          0.0         0.0            0.0           NaN    NaN   \n",
            "2          0.0          0.0         1.0            0.0           NaN    NaN   \n",
            "3          0.0          0.0         0.0            0.0           NaN    NaN   \n",
            "4          0.0          0.0         1.0            0.0           NaN    NaN   \n",
            "\n",
            "          username  num_followers  \\\n",
            "0   elginnewsround            852   \n",
            "1     pressjournal          69944   \n",
            "2  ArianeBurgessHI           3207   \n",
            "3  DominiqueBandet             90   \n",
            "4   martin85468119             18   \n",
            "\n",
            "                                     search_keywords  \\\n",
            "0  (900-block pandora avenue OR esquimalt OR high...   \n",
            "1  (900-block pandora avenue OR esquimalt OR high...   \n",
            "2  (900-block pandora avenue OR esquimalt OR high...   \n",
            "3  (900-block pandora avenue OR esquimalt OR high...   \n",
            "4  (900-block pandora avenue OR esquimalt OR high...   \n",
            "\n",
            "                                search_neighbourhood sentiment     score  \n",
            "0  900-block pandora avenue OR esquimalt OR highl...  positive  0.351921  \n",
            "1  900-block pandora avenue OR esquimalt OR highl...  positive  0.344510  \n",
            "2  900-block pandora avenue OR esquimalt OR highl...  positive  0.339811  \n",
            "3  900-block pandora avenue OR esquimalt OR highl...  positive  0.365494  \n",
            "4  900-block pandora avenue OR esquimalt OR highl...  positive  0.363048  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Check twitter data"
      ],
      "metadata": {
        "id": "TdELKFlfku-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ind = 8\n",
        "print(tweets_combined_df['search_neighbourhood'][ind])\n",
        "print(tweets_combined_df['search_keywords'][ind])\n",
        "print(tweets_combined_df['text'][ind])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Qon_G9kw5L",
        "outputId": "47c4183e-9242-472f-e819-06c367804d31"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "burnside-gorge OR fairfield-gonzales OR hollywood park OR north park OR pauquachin OR salt spring island OR stadacona park OR victoria\n",
            "(burnside-gorge OR fairfield-gonzales OR hollywood park OR north park OR pauquachin OR salt spring island OR stadacona park OR victoria) (anawin companion society OR safer victoria OR vtag OR mental health recovery partners, south island OR vancouver island mental health society OR greater victoria acting together OR the mustard seed OR yyj tenants union OR pacifica housing OR solid outreach OR housing OR camper) lang:en -is:retweet\n",
            "Can you trust @Dave_Eby?One week he says he's not buying in Burnside GorgeThen they announce he (as Housing Minister) \"bought a hotel (Capital City Centee) adjacent to Downtown\"Adjacent = Burnside Gorge@Adam_Stirling, what do you think?Can Eby be trusted? https://t.co/6yYCcXAvvC @mattdellok @Stephen_Andrew @BC_Housing @Dave_Eby @VictoriaDRA \"It would be easier to buy in Burnside Gorge\" Minister Eby https://t.co/bSP0LdgPwJ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained SetFit model\n",
        "model = SetFitModel.from_pretrained(\"sheilaflood/gvceh-setfit-rel-model2\")\n",
        "\n",
        "# Example text data\n",
        "texts = [\"Example text relevant to homelessness in Victoria.\", \"Irrelevant text about other topics.\"]\n",
        "\n",
        "# Model makes predictions\n",
        "predictions = model(texts)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLKtDWUjmJjk",
        "outputId": "46d5a558-3da5-49d5-c4eb-928e13ac9fe6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind = 7\n",
        "model(tweets_combined_df['text'][ind])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVJFGKxTvPe7",
        "outputId": "04ee94bf-c622-4033-dfce-a242ec219933"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Check that tweets are considered relevant by the model\n",
        "- all tweets were found to be relevant by the model"
      ],
      "metadata": {
        "id": "e1mySoBl3vsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to make predictions\n",
        "def get_prediction(text):\n",
        "    prediction = model([text])[0]\n",
        "    return prediction\n",
        "\n",
        "# # Apply the model to each row in the 'text' column with a progress bar\n",
        "# tqdm.pandas()  # Enable tqdm for pandas\n",
        "# tweets_combined_df['relevant'] = tweets_combined_df['text'].progress_apply(get_prediction)"
      ],
      "metadata": {
        "id": "eFNC5LeXvkLD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_integer_from_tensor(tensor_val):\n",
        "    return tensor_val.item()\n",
        "\n",
        "# Apply this function to the entire column\n",
        "# tweets_combined_df['relevant'] = tweets_combined_df['relevant'].apply(extract_integer_from_tensor)\n",
        "\n",
        "# Print value counts for the 'relevant' column\n",
        "# print(tweets_combined_df['relevant'].value_counts())"
      ],
      "metadata": {
        "id": "GnZ9swDM3T_I",
        "outputId": "186ecbf2-ce11-4c79-e5dc-0cafceca5423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'relevant'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3633b8c0d0eb>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print value counts for the 'relevant' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_combined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relevant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'relevant'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perform K-Means clustering of twitter data\n",
        "Generate embeddings of twitter posts"
      ],
      "metadata": {
        "id": "OP2tut9qupt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_embeddings = model_sent_transformer.encode(tweets_combined_df['text'].tolist(), show_progress_bar=True)"
      ],
      "metadata": {
        "id": "Va_ZK7zVcgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Run K-Means on twitter embeddings"
      ],
      "metadata": {
        "id": "Ke_ipLcWkD95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate sum of squared distances for different number of clusters\n",
        "Sum_of_squared_distances = []\n",
        "K = range(1,40)  # Adjust the range based on your dataset\n",
        "for k in K:\n",
        "    print(f\"Current k: {k}\")\n",
        "    km = KMeans(n_clusters=k, n_init=10)\n",
        "    km = km.fit(twitter_embeddings)\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "# Plot the elbow graph\n",
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Sum of squared distances')\n",
        "plt.title('Twitter Embeddings Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "btQMjfL5kHT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Assign clusters to embeddings"
      ],
      "metadata": {
        "id": "fKgXJWc7lZjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 15  # number of clusters\n",
        "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "kmeans.fit(twitter_embeddings)\n",
        "\n",
        "cluster_centroids = kmeans.cluster_centers_\n",
        "\n",
        "tweets_combined_df['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "XECpOJLolbAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_combined_df['cluster'] .unique())\n",
        "\n",
        "ind = 2\n",
        "cluster_tweets = tweets_combined_df[tweets_combined_df['cluster'] == ind].copy().reset_index(drop=True)\n",
        "for i in range(0, 10):\n",
        "  print(\"\\n---------------------Printing tweet\")\n",
        "  print(cluster_tweets['text'][i])"
      ],
      "metadata": {
        "id": "wC0U-1yTtDlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}