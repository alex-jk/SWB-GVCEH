{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQKRPOuNWKZnbhiy6pPfP3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/SWB-GVCEH/blob/main/models/relevance_model/Relevance_Model_Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfT3xHr4Euqq",
        "outputId": "eab8ab0c-e031-4f65-a6f2-36fdd75cedc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import os"
      ],
      "metadata": {
        "id": "mpNdU6QRGmtv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base URL for raw content in the GitHub repository\n",
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/models/relevance_model/'\n",
        "\n",
        "# Correctly encode the file names by replacing spaces with '%20'\n",
        "csv_file1 = 'GVCEH%20Milestone%202%20Labelling%201%20-%20RawData.csv'\n",
        "csv_file2 = 'GVCEH%20Milestone%202%20Labelling%202%20-%20RawData.csv'\n",
        "\n",
        "# Read the CSV files from GitHub\n",
        "df1 = pd.read_csv(base_url + csv_file1, usecols=['text', 'Relevant to Victoria', 'Relevant to Homelessness'])\n",
        "df2 = pd.read_csv(base_url + csv_file2, usecols=['text', 'Relevant to Victoria', 'Relevant to Homelessness'])\n",
        "\n",
        "# Rename columns for convenience\n",
        "df1 = df1.rename(columns={\"Relevant to Victoria\": \"vic\", \"Relevant to Homelessness\": \"hl\"})\n",
        "df2 = df2.rename(columns={\"Relevant to Victoria\": \"vic\", \"Relevant to Homelessness\": \"hl\"})\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df1.columns)\n",
        "print(df1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNpz9QPFGrxK",
        "outputId": "ec0d4ab8-c8ab-4b07-c0db-b1de0d01233d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['vic', 'hl', 'text'], dtype='object')\n",
            "   vic  hl                                               text\n",
            "0  Yes  No  @AnnaGreenwoodL1 @saanich Dawson Heights Housi...\n",
            "1   No  No  It's Election Day and the polls are now open u...\n",
            "2   No  No  Sidney Bulwer Michaelia Roger #彩票 Bblythe Camp...\n",
            "3   No  No  Me telling my parents I’m gonna spit on this o...\n",
            "4   No  No  WRD Director Joy Langford shared water conserv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Load the model\n",
        "model = AutoModel.from_pretrained(\"sheilaflood/gvceh-setfit-rel-model2\")\n",
        "\n",
        "# Check the configuration of the model\n",
        "config = model.config\n",
        "\n",
        "# Check the state dictionary (weights) of the model\n",
        "state_dict = model.state_dict()"
      ],
      "metadata": {
        "id": "jeSL6WwRZsdj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the configuration of the model\n",
        "print(model.config)\n",
        "\n",
        "# For example, to print the weights of the first transformer layer\n",
        "print(state_dict['encoder.layer.0.attention.self.query.weight'])"
      ],
      "metadata": {
        "id": "wJ0yd6GXaaER",
        "outputId": "5cf2a1d4-37b4-48ac-e960-7ace6876aa9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"sheilaflood/gvceh-setfit-rel-model2\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "tensor([[ 0.0748, -0.0012, -0.0722,  ...,  0.1540,  0.0737, -0.1073],\n",
            "        [-0.0733,  0.2006,  0.1053,  ...,  0.0542,  0.0490,  0.1169],\n",
            "        [ 0.1180,  0.0596, -0.0387,  ..., -0.0220, -0.0336,  0.1252],\n",
            "        ...,\n",
            "        [-0.1779,  0.0062, -0.0507,  ..., -0.0393,  0.0747, -0.0657],\n",
            "        [-0.2998,  0.0610,  0.0804,  ...,  0.0567, -0.0684,  0.0148],\n",
            "        [-0.1097, -0.0660,  0.1209,  ..., -0.2154,  0.0155, -0.0357]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/data/processed/twitter/github_actions/'\n",
        "csv_file1 = 'GVCEH-tweets-combined_2022-04-03.csv'\n",
        "df1 = pd.read_csv(base_url + csv_file1)\n",
        "\n",
        "print(df1.columns)\n",
        "print(df1.shape)\n",
        "print(df1.head(15))"
      ],
      "metadata": {
        "id": "Cs130_N2dRj5",
        "outputId": "d2ec585f-4b40-4e7c-a4a9-4dc22aa5b321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'text', 'scrape_time', 'tweet_id', 'created_at',\n",
            "       'reply_count', 'quote_count', 'like_count', 'retweet_count',\n",
            "       'geo_full_name', 'geo_id', 'username', 'num_followers',\n",
            "       'search_keywords', 'search_neighbourhood', 'sentiment', 'score'],\n",
            "      dtype='object')\n",
            "(8885, 17)\n",
            "    Unnamed: 0                                               text  \\\n",
            "0           23  @RogersCrispin @JinnealRobenko @Adam_Stirling ...   \n",
            "1           24  @citizens_vicbc As opposed to only having mult...   \n",
            "2           31  It's great to see Saanich Council taking actio...   \n",
            "3           32  @spaze_cadet @CStrable @chrislhayes Because of...   \n",
            "4           35  Ex-@BCLegislature Speaker Darryl Plecas reacts...   \n",
            "5           37  @KristaLoughton @CityOfVictoria Do you think t...   \n",
            "6           38  @GoVern2018 Hi Vernon. I assume you seen Victo...   \n",
            "7           40  Saanich homeowners won't need council approval...   \n",
            "8           45  RT @VicBuilders: @tim3048 @TristinHopper Same ...   \n",
            "9           46  @tim3048 @TristinHopper Same for building perm...   \n",
            "10          49  Brian O’Donnell from the Homeownership Bureau ...   \n",
            "11          59  She also helped to promote the Mentor/Apprenti...   \n",
            "12          62  On behalf of the Victoria Foundation board, ho...   \n",
            "13          63  She was an active volunteer for many other org...   \n",
            "14          64  We are sad to learn of the passing of Fiona Ma...   \n",
            "\n",
            "                   scrape_time             tweet_id  \\\n",
            "0   2022-07-13 14:35:48.363408  1547091087562399744   \n",
            "1   2022-07-13 14:35:54.472304  1546966233739698176   \n",
            "2   2022-07-13 14:37:26.139494  1547258750267863040   \n",
            "3   2022-07-13 14:37:26.139494  1547248807384981504   \n",
            "4   2022-07-13 14:37:26.139494  1547073298382479360   \n",
            "5   2022-07-13 14:37:26.139494  1547049561230700544   \n",
            "6   2022-07-13 14:37:26.139494  1547048644074819584   \n",
            "7   2022-07-13 14:37:26.139494  1547036775016800256   \n",
            "8   2022-07-13 14:37:26.222325  1546989535497797632   \n",
            "9   2022-07-13 14:37:26.222325  1546981645911072768   \n",
            "10  2022-07-13 14:37:26.222325  1546946487313207296   \n",
            "11  2022-07-13 14:38:16.631770  1547313735227699200   \n",
            "12  2022-07-13 14:38:16.631770  1547288213621837824   \n",
            "13  2022-07-13 14:38:16.631770  1547288102627975168   \n",
            "14  2022-07-13 14:38:16.631770  1547288014891585536   \n",
            "\n",
            "                   created_at  reply_count  quote_count  like_count  \\\n",
            "0   2022-07-13 05:30:33+00:00            1            0           0   \n",
            "1   2022-07-12 21:14:26+00:00            0            0           1   \n",
            "2   2022-07-13 16:36:47+00:00            1            0           5   \n",
            "3   2022-07-13 15:57:16+00:00            0            0           0   \n",
            "4   2022-07-13 04:19:52+00:00            1            0           3   \n",
            "5   2022-07-13 02:45:32+00:00            1            0           0   \n",
            "6   2022-07-13 02:41:54+00:00            1            0           2   \n",
            "7   2022-07-13 01:54:44+00:00            0            0           2   \n",
            "8   2022-07-12 22:47:01+00:00            0            0           0   \n",
            "9   2022-07-12 22:15:40+00:00            0            0           0   \n",
            "10  2022-07-12 19:55:58+00:00            1            0           0   \n",
            "11  2022-07-13 20:15:16+00:00            1            0           0   \n",
            "12  2022-07-13 18:33:52+00:00            0            0           1   \n",
            "13  2022-07-13 18:33:25+00:00            1            0           1   \n",
            "14  2022-07-13 18:33:04+00:00            1            1           1   \n",
            "\n",
            "    retweet_count               geo_full_name            geo_id  \\\n",
            "0               0                         NaN               NaN   \n",
            "1               0                         NaN               NaN   \n",
            "2               0                         NaN               NaN   \n",
            "3               0                         NaN               NaN   \n",
            "4               2                         NaN               NaN   \n",
            "5               0  Victoria, British Columbia  4fdbcad8c3ed7790   \n",
            "6               0                         NaN               NaN   \n",
            "7               1                         NaN               NaN   \n",
            "8               0                         NaN               NaN   \n",
            "9               0                         NaN               NaN   \n",
            "10              0                         NaN               NaN   \n",
            "11              0                         NaN               NaN   \n",
            "12              0                         NaN               NaN   \n",
            "13              0                         NaN               NaN   \n",
            "14              0                         NaN               NaN   \n",
            "\n",
            "           username  num_followers  \\\n",
            "0     JohnsonStBRDG            482   \n",
            "1     bot_bites_bob            165   \n",
            "2     BasilLangevin            553   \n",
            "3      ssfb85263085             21   \n",
            "4    theBreakerNews           5915   \n",
            "5        ja10663725              0   \n",
            "6        krogher_77            193   \n",
            "7        JackieNgai           1446   \n",
            "8     VerandaBlonde             41   \n",
            "9        aryssadocs             10   \n",
            "10  TheBackPackPro1            774   \n",
            "11   KristaLoughton           1781   \n",
            "12    WBAFoundation          13113   \n",
            "13       HelpTheVic            410   \n",
            "14        iyesf_org             19   \n",
            "\n",
            "                                      search_keywords  \\\n",
            "0   (langford OR victoria OR fairfield-gonzales OR...   \n",
            "1   (langford OR victoria OR fairfield-gonzales OR...   \n",
            "2   (saanich OR uplands OR quadra village OR centr...   \n",
            "3   (saanich OR uplands OR quadra village OR centr...   \n",
            "4   (saanich OR uplands OR quadra village OR centr...   \n",
            "5   (saanich OR uplands OR quadra village OR centr...   \n",
            "6   (saanich OR uplands OR quadra village OR centr...   \n",
            "7   (saanich OR uplands OR quadra village OR centr...   \n",
            "8   (saanich OR uplands OR quadra village OR centr...   \n",
            "9   (saanich OR uplands OR quadra village OR centr...   \n",
            "10  (saanich OR uplands OR quadra village OR centr...   \n",
            "11  Victoria (aceh OR city of langford OR greater ...   \n",
            "12  Victoria (aceh OR city of langford OR greater ...   \n",
            "13  Victoria (aceh OR city of langford OR greater ...   \n",
            "14  Victoria (aceh OR city of langford OR greater ...   \n",
            "\n",
            "                                 search_neighbourhood sentiment  score  \n",
            "0   langford OR victoria OR fairfield-gonzales OR ...   Neutral      0  \n",
            "1   langford OR victoria OR fairfield-gonzales OR ...   Neutral      0  \n",
            "2   saanich OR uplands OR quadra village OR centra...  Positive      0  \n",
            "3   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "4   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "5   saanich OR uplands OR quadra village OR centra...  Negative      0  \n",
            "6   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "7   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "8   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "9   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "10  saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "11                                           Victoria  Positive      0  \n",
            "12                                           Victoria  Positive      0  \n",
            "13                                           Victoria   Neutral      0  \n",
            "14                                           Victoria  Positive      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.text[0])"
      ],
      "metadata": {
        "id": "Ss1Ay7KyhScJ",
        "outputId": "998b4fcc-7fbe-415e-94c8-bd876f311176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@RogersCrispin @JinnealRobenko @Adam_Stirling @timescolonist Sure and not cancelling affordable housing projects in the 80’s, not inducing urban sprawl in Langford, and prioritizing smart urban density instead of doing Gordon Head all could have been better… but we have what we have and have to find a way forward given the circumstance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setfit"
      ],
      "metadata": {
        "id": "2dsQ-gLQsUql",
        "outputId": "8a042ec9-ddfb-4869-968e-38382d5b59e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setfit\n",
            "  Downloading setfit-0.7.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.3.0 (from setfit)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.1 (from setfit)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate>=0.3.0 (from setfit)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.3.0->setfit)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.3.0->setfit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.3.0->setfit)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate>=0.3.0->setfit)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=2.2.1->setfit)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets>=2.3.0->setfit) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets>=2.3.0->setfit) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.1->setfit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.1->setfit) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.1->setfit) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.1->setfit) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.3.0->setfit) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=92019a3996b102b02a8df8f704d9baac7e718af2f6e4e074aab7caa0ff3b611d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, pyarrow-hotfix, dill, responses, multiprocess, datasets, sentence-transformers, evaluate, setfit\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 setfit-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel\n",
        "\n",
        "# Load the pretrained SetFit model\n",
        "model = SetFitModel.from_pretrained(\"sheilaflood/gvceh-setfit-rel-model2\")\n",
        "\n",
        "# Example text data\n",
        "texts = [\"Example text relevant to homelessness in Victoria.\", \"Irrelevant text about other topics.\"]\n",
        "\n",
        "# Model makes predictions\n",
        "predictions = model(texts)\n",
        "print(predictions)\n",
        "# The predictions would be an array of binary labels, such as [1, 0]"
      ],
      "metadata": {
        "id": "qprghE08sGVp",
        "outputId": "3b8a2436-8a60-47d6-c82d-1b1c330691f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0])\n"
          ]
        }
      ]
    }
  ]
}