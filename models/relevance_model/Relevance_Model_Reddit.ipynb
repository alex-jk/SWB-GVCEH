{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD3WFmjzi3mvNamVk1eHXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/SWB-GVCEH/blob/main/models/relevance_model/Relevance_Model_Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfT3xHr4Euqq",
        "outputId": "eab8ab0c-e031-4f65-a6f2-36fdd75cedc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import os"
      ],
      "metadata": {
        "id": "mpNdU6QRGmtv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base URL for raw content in the GitHub repository\n",
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/models/relevance_model/'\n",
        "\n",
        "# Correctly encode the file names by replacing spaces with '%20'\n",
        "csv_file1 = 'GVCEH%20Milestone%202%20Labelling%201%20-%20RawData.csv'\n",
        "csv_file2 = 'GVCEH%20Milestone%202%20Labelling%202%20-%20RawData.csv'\n",
        "\n",
        "# Read the CSV files from GitHub\n",
        "df1 = pd.read_csv(base_url + csv_file1, usecols=['text', 'Relevant to Victoria', 'Relevant to Homelessness'])\n",
        "df2 = pd.read_csv(base_url + csv_file2, usecols=['text', 'Relevant to Victoria', 'Relevant to Homelessness'])\n",
        "\n",
        "# Rename columns for convenience\n",
        "df1 = df1.rename(columns={\"Relevant to Victoria\": \"vic\", \"Relevant to Homelessness\": \"hl\"})\n",
        "df2 = df2.rename(columns={\"Relevant to Victoria\": \"vic\", \"Relevant to Homelessness\": \"hl\"})\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df1.columns)\n",
        "print(df1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNpz9QPFGrxK",
        "outputId": "ec0d4ab8-c8ab-4b07-c0db-b1de0d01233d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['vic', 'hl', 'text'], dtype='object')\n",
            "   vic  hl                                               text\n",
            "0  Yes  No  @AnnaGreenwoodL1 @saanich Dawson Heights Housi...\n",
            "1   No  No  It's Election Day and the polls are now open u...\n",
            "2   No  No  Sidney Bulwer Michaelia Roger #彩票 Bblythe Camp...\n",
            "3   No  No  Me telling my parents I’m gonna spit on this o...\n",
            "4   No  No  WRD Director Joy Langford shared water conserv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Load the model\n",
        "model = AutoModel.from_pretrained(\"sheilaflood/gvceh-setfit-rel-model2\")\n",
        "\n",
        "# Check the configuration of the model\n",
        "config = model.config\n",
        "\n",
        "# Check the state dictionary (weights) of the model\n",
        "state_dict = model.state_dict()"
      ],
      "metadata": {
        "id": "jeSL6WwRZsdj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the configuration of the model\n",
        "print(model.config)\n",
        "\n",
        "# For example, to print the weights of the first transformer layer\n",
        "print(state_dict['encoder.layer.0.attention.self.query.weight'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ0yd6GXaaER",
        "outputId": "5cf2a1d4-37b4-48ac-e960-7ace6876aa9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"sheilaflood/gvceh-setfit-rel-model2\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "tensor([[ 0.0748, -0.0012, -0.0722,  ...,  0.1540,  0.0737, -0.1073],\n",
            "        [-0.0733,  0.2006,  0.1053,  ...,  0.0542,  0.0490,  0.1169],\n",
            "        [ 0.1180,  0.0596, -0.0387,  ..., -0.0220, -0.0336,  0.1252],\n",
            "        ...,\n",
            "        [-0.1779,  0.0062, -0.0507,  ..., -0.0393,  0.0747, -0.0657],\n",
            "        [-0.2998,  0.0610,  0.0804,  ...,  0.0567, -0.0684,  0.0148],\n",
            "        [-0.1097, -0.0660,  0.1209,  ..., -0.2154,  0.0155, -0.0357]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/data/processed/twitter/github_actions/'\n",
        "csv_file1 = 'GVCEH-tweets-combined_2022-04-03.csv'\n",
        "df1 = pd.read_csv(base_url + csv_file1)\n",
        "\n",
        "print(df1.columns)\n",
        "print(df1.shape)\n",
        "print(df1.head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs130_N2dRj5",
        "outputId": "d2ec585f-4b40-4e7c-a4a9-4dc22aa5b321"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'text', 'scrape_time', 'tweet_id', 'created_at',\n",
            "       'reply_count', 'quote_count', 'like_count', 'retweet_count',\n",
            "       'geo_full_name', 'geo_id', 'username', 'num_followers',\n",
            "       'search_keywords', 'search_neighbourhood', 'sentiment', 'score'],\n",
            "      dtype='object')\n",
            "(8885, 17)\n",
            "    Unnamed: 0                                               text  \\\n",
            "0           23  @RogersCrispin @JinnealRobenko @Adam_Stirling ...   \n",
            "1           24  @citizens_vicbc As opposed to only having mult...   \n",
            "2           31  It's great to see Saanich Council taking actio...   \n",
            "3           32  @spaze_cadet @CStrable @chrislhayes Because of...   \n",
            "4           35  Ex-@BCLegislature Speaker Darryl Plecas reacts...   \n",
            "5           37  @KristaLoughton @CityOfVictoria Do you think t...   \n",
            "6           38  @GoVern2018 Hi Vernon. I assume you seen Victo...   \n",
            "7           40  Saanich homeowners won't need council approval...   \n",
            "8           45  RT @VicBuilders: @tim3048 @TristinHopper Same ...   \n",
            "9           46  @tim3048 @TristinHopper Same for building perm...   \n",
            "10          49  Brian O’Donnell from the Homeownership Bureau ...   \n",
            "11          59  She also helped to promote the Mentor/Apprenti...   \n",
            "12          62  On behalf of the Victoria Foundation board, ho...   \n",
            "13          63  She was an active volunteer for many other org...   \n",
            "14          64  We are sad to learn of the passing of Fiona Ma...   \n",
            "\n",
            "                   scrape_time             tweet_id  \\\n",
            "0   2022-07-13 14:35:48.363408  1547091087562399744   \n",
            "1   2022-07-13 14:35:54.472304  1546966233739698176   \n",
            "2   2022-07-13 14:37:26.139494  1547258750267863040   \n",
            "3   2022-07-13 14:37:26.139494  1547248807384981504   \n",
            "4   2022-07-13 14:37:26.139494  1547073298382479360   \n",
            "5   2022-07-13 14:37:26.139494  1547049561230700544   \n",
            "6   2022-07-13 14:37:26.139494  1547048644074819584   \n",
            "7   2022-07-13 14:37:26.139494  1547036775016800256   \n",
            "8   2022-07-13 14:37:26.222325  1546989535497797632   \n",
            "9   2022-07-13 14:37:26.222325  1546981645911072768   \n",
            "10  2022-07-13 14:37:26.222325  1546946487313207296   \n",
            "11  2022-07-13 14:38:16.631770  1547313735227699200   \n",
            "12  2022-07-13 14:38:16.631770  1547288213621837824   \n",
            "13  2022-07-13 14:38:16.631770  1547288102627975168   \n",
            "14  2022-07-13 14:38:16.631770  1547288014891585536   \n",
            "\n",
            "                   created_at  reply_count  quote_count  like_count  \\\n",
            "0   2022-07-13 05:30:33+00:00            1            0           0   \n",
            "1   2022-07-12 21:14:26+00:00            0            0           1   \n",
            "2   2022-07-13 16:36:47+00:00            1            0           5   \n",
            "3   2022-07-13 15:57:16+00:00            0            0           0   \n",
            "4   2022-07-13 04:19:52+00:00            1            0           3   \n",
            "5   2022-07-13 02:45:32+00:00            1            0           0   \n",
            "6   2022-07-13 02:41:54+00:00            1            0           2   \n",
            "7   2022-07-13 01:54:44+00:00            0            0           2   \n",
            "8   2022-07-12 22:47:01+00:00            0            0           0   \n",
            "9   2022-07-12 22:15:40+00:00            0            0           0   \n",
            "10  2022-07-12 19:55:58+00:00            1            0           0   \n",
            "11  2022-07-13 20:15:16+00:00            1            0           0   \n",
            "12  2022-07-13 18:33:52+00:00            0            0           1   \n",
            "13  2022-07-13 18:33:25+00:00            1            0           1   \n",
            "14  2022-07-13 18:33:04+00:00            1            1           1   \n",
            "\n",
            "    retweet_count               geo_full_name            geo_id  \\\n",
            "0               0                         NaN               NaN   \n",
            "1               0                         NaN               NaN   \n",
            "2               0                         NaN               NaN   \n",
            "3               0                         NaN               NaN   \n",
            "4               2                         NaN               NaN   \n",
            "5               0  Victoria, British Columbia  4fdbcad8c3ed7790   \n",
            "6               0                         NaN               NaN   \n",
            "7               1                         NaN               NaN   \n",
            "8               0                         NaN               NaN   \n",
            "9               0                         NaN               NaN   \n",
            "10              0                         NaN               NaN   \n",
            "11              0                         NaN               NaN   \n",
            "12              0                         NaN               NaN   \n",
            "13              0                         NaN               NaN   \n",
            "14              0                         NaN               NaN   \n",
            "\n",
            "           username  num_followers  \\\n",
            "0     JohnsonStBRDG            482   \n",
            "1     bot_bites_bob            165   \n",
            "2     BasilLangevin            553   \n",
            "3      ssfb85263085             21   \n",
            "4    theBreakerNews           5915   \n",
            "5        ja10663725              0   \n",
            "6        krogher_77            193   \n",
            "7        JackieNgai           1446   \n",
            "8     VerandaBlonde             41   \n",
            "9        aryssadocs             10   \n",
            "10  TheBackPackPro1            774   \n",
            "11   KristaLoughton           1781   \n",
            "12    WBAFoundation          13113   \n",
            "13       HelpTheVic            410   \n",
            "14        iyesf_org             19   \n",
            "\n",
            "                                      search_keywords  \\\n",
            "0   (langford OR victoria OR fairfield-gonzales OR...   \n",
            "1   (langford OR victoria OR fairfield-gonzales OR...   \n",
            "2   (saanich OR uplands OR quadra village OR centr...   \n",
            "3   (saanich OR uplands OR quadra village OR centr...   \n",
            "4   (saanich OR uplands OR quadra village OR centr...   \n",
            "5   (saanich OR uplands OR quadra village OR centr...   \n",
            "6   (saanich OR uplands OR quadra village OR centr...   \n",
            "7   (saanich OR uplands OR quadra village OR centr...   \n",
            "8   (saanich OR uplands OR quadra village OR centr...   \n",
            "9   (saanich OR uplands OR quadra village OR centr...   \n",
            "10  (saanich OR uplands OR quadra village OR centr...   \n",
            "11  Victoria (aceh OR city of langford OR greater ...   \n",
            "12  Victoria (aceh OR city of langford OR greater ...   \n",
            "13  Victoria (aceh OR city of langford OR greater ...   \n",
            "14  Victoria (aceh OR city of langford OR greater ...   \n",
            "\n",
            "                                 search_neighbourhood sentiment  score  \n",
            "0   langford OR victoria OR fairfield-gonzales OR ...   Neutral      0  \n",
            "1   langford OR victoria OR fairfield-gonzales OR ...   Neutral      0  \n",
            "2   saanich OR uplands OR quadra village OR centra...  Positive      0  \n",
            "3   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "4   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "5   saanich OR uplands OR quadra village OR centra...  Negative      0  \n",
            "6   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "7   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "8   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "9   saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "10  saanich OR uplands OR quadra village OR centra...   Neutral      0  \n",
            "11                                           Victoria  Positive      0  \n",
            "12                                           Victoria  Positive      0  \n",
            "13                                           Victoria   Neutral      0  \n",
            "14                                           Victoria  Positive      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss1Ay7KyhScJ",
        "outputId": "998b4fcc-7fbe-415e-94c8-bd876f311176"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@RogersCrispin @JinnealRobenko @Adam_Stirling @timescolonist Sure and not cancelling affordable housing projects in the 80’s, not inducing urban sprawl in Langford, and prioritizing smart urban density instead of doing Gordon Head all could have been better… but we have what we have and have to find a way forward given the circumstance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setfit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dsQ-gLQsUql",
        "outputId": "8a042ec9-ddfb-4869-968e-38382d5b59e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setfit\n",
            "  Downloading setfit-0.7.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.3.0 (from setfit)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.1 (from setfit)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate>=0.3.0 (from setfit)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.3.0->setfit)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.3.0->setfit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.3.0->setfit)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.3.0->setfit) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate>=0.3.0->setfit)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.1->setfit) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=2.2.1->setfit)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.3.0->setfit) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets>=2.3.0->setfit) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets>=2.3.0->setfit) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.3.0->setfit) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.1->setfit) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.1->setfit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.1->setfit) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.3.0->setfit) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.1->setfit) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.1->setfit) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.3.0->setfit) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.1->setfit) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=92019a3996b102b02a8df8f704d9baac7e718af2f6e4e074aab7caa0ff3b611d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, pyarrow-hotfix, dill, responses, multiprocess, datasets, sentence-transformers, evaluate, setfit\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 setfit-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel\n",
        "\n",
        "# Load the pretrained SetFit model\n",
        "model = SetFitModel.from_pretrained(\"sheilaflood/gvceh-setfit-rel-model2\")\n",
        "\n",
        "# Example text data\n",
        "texts = [\"Example text relevant to homelessness in Victoria.\", \"Irrelevant text about other topics.\"]\n",
        "\n",
        "# Model makes predictions\n",
        "predictions = model(texts)\n",
        "print(predictions)\n",
        "# The predictions would be an array of binary labels, such as [1, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qprghE08sGVp",
        "outputId": "3b8a2436-8a60-47d6-c82d-1b1c330691f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/SWB-GVCEH/reddit_data_search_terms.csv'  # Update with the path to your CSV file in Google Drive\n",
        "reddit_data_search_df = pd.read_csv(file_path)\n",
        "print(reddit_data_search_df.shape)\n",
        "print(reddit_data_search_df.columns)\n",
        "print(reddit_data_search_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yORJrhaTs8Ns",
        "outputId": "8add27f2-cf3c-48ae-d72a-a82bf257f649"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(152161, 6)\n",
            "Index(['Subreddit', 'Title', 'Text', 'User ID', 'Comment', 'Search Term'], dtype='object')\n",
            "   Subreddit                                              Title  \\\n",
            "0      Sooke                                   Camping in Sooke   \n",
            "1      Sooke                                   Camping in Sooke   \n",
            "2      Sooke                                   Camping in Sooke   \n",
            "3      Sooke                                   Camping in Sooke   \n",
            "4      Sooke  Sooke boy denied $19,000 per month drug for th...   \n",
            "5      Sooke        Are power outages in Sooke a regular thing?   \n",
            "6      Sooke        Are power outages in Sooke a regular thing?   \n",
            "7      Sooke        Are power outages in Sooke a regular thing?   \n",
            "8      Sooke        Are power outages in Sooke a regular thing?   \n",
            "9      Sooke        Are power outages in Sooke a regular thing?   \n",
            "10     Sooke  Help: What’s the best option for housing right...   \n",
            "11     Sooke  Help: What’s the best option for housing right...   \n",
            "12     Sooke  Help: What’s the best option for housing right...   \n",
            "13     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "14     Sooke  Cockatoo escaped across from Saseenos. Please ...   \n",
            "15     Sooke  Cockatoo escaped across from Saseenos. Please ...   \n",
            "16     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "17     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "18     Sooke  Help: What’s the best option for housing right...   \n",
            "19     Sooke  Help: What’s the best option for housing right...   \n",
            "\n",
            "                                                 Text         User ID  \\\n",
            "0   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "1   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "2   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "3   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "4                                                 NaN  TrueNorthGreen   \n",
            "5   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "6   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "7   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "8   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "9   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "10  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "11  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "12  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "13                                                NaN     AMadcapLass   \n",
            "14  3 days ago my friends cockatoo escaped from he...        babetteq   \n",
            "15  3 days ago my friends cockatoo escaped from he...        babetteq   \n",
            "16                                                NaN     AMadcapLass   \n",
            "17                                                NaN     AMadcapLass   \n",
            "18  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "19  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "\n",
            "                                              Comment  \\\n",
            "0   Check on the BC parks website to see if China ...   \n",
            "1   Just a heads up that there is a government man...   \n",
            "2   Check on the BC parks website to see if China ...   \n",
            "3   Just a heads up that there is a government man...   \n",
            "4   there are other treatments for this, i'm just ...   \n",
            "5   Lived here for a year. The power has probably ...   \n",
            "6   It depends where you’re at. In Sooke core, sun...   \n",
            "7   Yes, expect your power to go out several times...   \n",
            "8   Ah that's all good to know! Kinda what I'd exp...   \n",
            "9   Summer it’s rare, winter months it’s very common.   \n",
            "10  About the same options for those of us already...   \n",
            "11                                          [deleted]   \n",
            "12  Its better to buy than rent if you can afford ...   \n",
            "13  Would you post an address and contact phone nu...   \n",
            "14     I'll keep my eyes and ears open; best of luck!   \n",
            "15  Thank you.  Today was first of no sightings. W...   \n",
            "16  Would you post an address and contact phone nu...   \n",
            "17  Would you post an address and contact phone nu...   \n",
            "18  About the same options for those of us already...   \n",
            "19                                          [deleted]   \n",
            "\n",
            "                       Search Term  \n",
            "0                             Camp  \n",
            "1                             Camp  \n",
            "2                          Camping  \n",
            "3                          Camping  \n",
            "4                            Drugs  \n",
            "5                             Home  \n",
            "6                             Home  \n",
            "7                             Home  \n",
            "8                             Home  \n",
            "9                             Home  \n",
            "10                         Housing  \n",
            "11                         Housing  \n",
            "12                         Housing  \n",
            "13                         Housing  \n",
            "14                         Housing  \n",
            "15                         Housing  \n",
            "16         Makola Housing Society   \n",
            "17  Sooke Transition House Society  \n",
            "18  Sooke Transition House Society  \n",
            "19  Sooke Transition House Society  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 'Title' and 'Text'\n",
        "reddit_data_search_df['TitleText'] = reddit_data_search_df['Title'] + ' ' + reddit_data_search_df['Text']\n",
        "\n",
        "ind = 10\n",
        "select_text = reddit_data_search_df['TitleText'][ind]\n",
        "print(select_text)\n",
        "print(model(select_text))\n",
        "\n",
        "print(reddit_data_search_df['Comment'][ind])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7rEhqtq6xrz",
        "outputId": "b03fcb67-40fd-4bfc-bbf6-ca4b88f7c280"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help: What’s the best option for housing right now for someone about to move from out of province to Sooke for a job? Looking for at least a two bedroom, preferably a house than a condo. Is it better to rent or buy? Or live somewhere else and commute?\n",
            "tensor(1)\n",
            "About the same options for those of us already living here. Nothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_title_text_df = reddit_data_search_df[['Subreddit', 'User ID', 'TitleText']].drop_duplicates().reset_index(drop=True)\n",
        "print(unique_title_text_df.shape)\n",
        "\n",
        "num_nan_titletext = unique_title_text_df['TitleText'].isna().sum()\n",
        "print(f\"Number of rows with NA TitleText: {num_nan_titletext}\")\n",
        "\n",
        "print(unique_title_text_df.head(10))"
      ],
      "metadata": {
        "id": "XLp25oIREAN8",
        "outputId": "2908a0e8-9933-4725-9721-a25ac72a6216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4453, 3)\n",
            "Number of rows with NA TitleText: 723\n",
            "    Subreddit         User ID  \\\n",
            "0       Sooke    VanillaWrong   \n",
            "1       Sooke  TrueNorthGreen   \n",
            "2       Sooke         Pkard82   \n",
            "3       Sooke     redd_planet   \n",
            "4       Sooke     AMadcapLass   \n",
            "5       Sooke        babetteq   \n",
            "6       Sooke    scoobysmokes   \n",
            "7  VictoriaBC   Ill_Kale_4507   \n",
            "8  VictoriaBC    rednightmare   \n",
            "9  VictoriaBC         LukasWE   \n",
            "\n",
            "                                           TitleText  \n",
            "0  Camping in Sooke Hi, we’re planning on camping...  \n",
            "1                                                NaN  \n",
            "2  Are power outages in Sooke a regular thing? Gr...  \n",
            "3  Help: What’s the best option for housing right...  \n",
            "4                                                NaN  \n",
            "5  Cockatoo escaped across from Saseenos. Please ...  \n",
            "6  Are there sexual predator watchdog groups in S...  \n",
            "7  True change around homelessness from the homel...  \n",
            "8                                                NaN  \n",
            "9  Victoria needs to end the compassionate approa...  \n"
          ]
        }
      ]
    }
  ]
}