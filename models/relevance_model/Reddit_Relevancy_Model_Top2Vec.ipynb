{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv7xSPhOrpOrN1mmLo42yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/SWB-GVCEH/blob/main/models/relevance_model/Reddit_Relevancy_Model_Top2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reddit Relevancy Model - Topic Modeling - Top2Vec"
      ],
      "metadata": {
        "id": "hzG23BD7mGjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmoXSjxjmsAX",
        "outputId": "b5c03b6d-2340-48ad-f327-345a215014ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install top2vec\n",
        "!pip install top2vec[sentence_encoders]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLEbrKB3Ued1",
        "outputId": "96add742-81f3-4e51-fbaf-30a7ed286fbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec in /usr/local/lib/python3.10/dist-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from top2vec) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.2.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from top2vec) (0.5.6)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.10/dist-packages (from top2vec) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.9.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.27->top2vec) (0.29.37)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.27->top2vec) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->top2vec) (3.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2024.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec) (3.7.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->top2vec) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (3.1.2)\n",
            "Requirement already satisfied: top2vec[sentence_encoders] in /usr/local/lib/python3.10/dist-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (1.2.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (0.5.6)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (1.9.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from top2vec[sentence_encoders]) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.37)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->top2vec[sentence_encoders]) (3.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec[sentence_encoders]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec[sentence_encoders]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec[sentence_encoders]) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.36.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub->top2vec[sentence_encoders]) (2.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec[sentence_encoders]) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec[sentence_encoders]) (3.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (0.0.7)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (0.11.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->top2vec[sentence_encoders]) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow->top2vec[sentence_encoders]) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow->top2vec[sentence_encoders]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow->top2vec[sentence_encoders]) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow->top2vec[sentence_encoders]) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow->top2vec[sentence_encoders]) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from top2vec import Top2Vec\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "Lma2EwUwnNVk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Import the full Reddit dataset\n",
        "- remove duplicates to make sure that posts are unqiue\n",
        "- TitleText is the text column of interest"
      ],
      "metadata": {
        "id": "Epzfop1rmjnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXMK0WNLlWFI",
        "outputId": "e9fe5ad5-28cf-4491-bba0-f357c7add48a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "cd_test shape: (11160, 23)\n",
            "\n",
            "--------------- Columns: Index(['index', 'Subreddit', 'Title', 'Text', 'TitleText', 'relevance_score',\n",
            "       'most_common_centroid_id', 'top_terms_from_centroid',\n",
            "       'topics_from_centroid', 'Score_model2', 'label_model2', 'label_model1',\n",
            "       'relevant_sentences', 'topic_num', 'Relevant_document',\n",
            "       'Relevant_topic', 'topic_label', 'Sentiment_Full',\n",
            "       'Sentence_Level_Sentiment_Compund',\n",
            "       'Relevent_Sentence_Sentiment_Compund', 'BERT_sentiment_all',\n",
            "       'BERT_sentiments_relevant_sentences', 'manual_label'],\n",
            "      dtype='object')\n",
            "\n",
            "------------------------------\n",
            "    Subreddit                                              Title  \\\n",
            "0  VictoriaBC  True change around homelessness from the homel...   \n",
            "1  VictoriaBC  New transitional housing facility on Douglas o...   \n",
            "2  VictoriaBC   Where to buy a reasonably priced Christmas tree?   \n",
            "3  VictoriaBC  Looking for a Christmas tree? Please consider ...   \n",
            "4  VictoriaBC                              Free clothing places?   \n",
            "\n",
            "                                                Text  \\\n",
            "0  I've started my own organization to expose the...   \n",
            "1                                               None   \n",
            "2  Went to the Christmas tree farm today and thou...   \n",
            "3                                               None   \n",
            "4  Hey so long story short I moved here from Sask...   \n",
            "\n",
            "                                           TitleText  \n",
            "0  True change around homelessness from the homel...  \n",
            "1  New transitional housing facility on Douglas o...  \n",
            "2  Where to buy a reasonably priced Christmas tre...  \n",
            "3  Looking for a Christmas tree? Please consider ...  \n",
            "4  Free clothing places?. Hey so long story short...  \n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/drive/My Drive/SWB-GVCEH/Complete_Data_v3.json'\n",
        "reddit_data_df = pd.read_json(file_path)\n",
        "\n",
        "print(f\"\\ncd_test shape: {reddit_data_df.shape}\")\n",
        "print(f\"\\n--------------- Columns: {reddit_data_df.columns}\")\n",
        "\n",
        "select_cols = ['Subreddit', 'Title', 'Text', 'TitleText']\n",
        "reddit_data_df = reddit_data_df[select_cols]\n",
        "reddit_data_df.drop_duplicates(inplace=True)\n",
        "reddit_data_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\n------------------------------\")\n",
        "print(reddit_data_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data_df['TitleText_fmtd'] = reddit_data_df['TitleText'].apply(lambda x: re.sub(r'\\n', ' ', x))\n",
        "\n",
        "docs = reddit_data_df['TitleText_fmtd'].to_list()\n",
        "docs = [re.sub(r'[^a-zA-Z0-9\\s]+', '',d).replace(\" .\",\".\") for d in docs]\n",
        "docs = [re.sub(r'http\\S+', '', d) for d in docs]\n",
        "\n",
        "docs[100]"
      ],
      "metadata": {
        "id": "zEWYrgNNmFvN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8a1f4f19-e6f6-45ec-f15e-3e634efdffab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What do the wealthy in VictoriaBC do Wealthy neighbourhoods like uplands north saanich 10 mile point etc I see amazing homes and but they are also very secluded from the rest of the general public in Victoria you would need a boat to even see the houses properly   What do they do which is extremely profitable in victoria BC Where do they hang out What community  events can they be seen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Top2Vec.__doc__)"
      ],
      "metadata": {
        "id": "aDnCoguxVPs-",
        "outputId": "1ffc4a7a-ef4b-4f2b-c755-8f3a353d6fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Top2Vec\n",
            "\n",
            "    Creates jointly embedded topic, document and word vectors.\n",
            "\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    documents: List of str\n",
            "        Input corpus, should be a list of strings.\n",
            "\n",
            "    min_count: int (Optional, default 50)\n",
            "        Ignores all words with total frequency lower than this. For smaller\n",
            "        corpora a smaller min_count will be necessary.\n",
            "\n",
            "    topic_merge_delta: float (default 0.1)\n",
            "        Merges topic vectors which have a cosine distance smaller than\n",
            "        topic_merge_delta using dbscan. The epsilon parameter of dbscan is\n",
            "        set to the topic_merge_delta.\n",
            "\n",
            "    ngram_vocab: bool (Optional, default False)\n",
            "        Add phrases to topic descriptions.\n",
            "\n",
            "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
            "        to the vocabulary.\n",
            "\n",
            "        For more information visit:\n",
            "        https://radimrehurek.com/gensim/models/phrases.html\n",
            "\n",
            "    ngram_vocab_args: dict (Optional, default None)\n",
            "        Pass custom arguments to gensim phrases.\n",
            "\n",
            "        For more information visit:\n",
            "        https://radimrehurek.com/gensim/models/phrases.html\n",
            "\n",
            "    embedding_model: string or callable\n",
            "        This will determine which model is used to generate the document and\n",
            "        word embeddings. The valid string options are:\n",
            "\n",
            "            * doc2vec\n",
            "            * universal-sentence-encoder\n",
            "            * universal-sentence-encoder-large\n",
            "            * universal-sentence-encoder-multilingual\n",
            "            * universal-sentence-encoder-multilingual-large\n",
            "            * distiluse-base-multilingual-cased\n",
            "            * all-MiniLM-L6-v2\n",
            "            * paraphrase-multilingual-MiniLM-L12-v2\n",
            "\n",
            "        For large data sets and data sets with very unique vocabulary doc2vec\n",
            "        could produce better results. This will train a doc2vec model from\n",
            "        scratch. This method is language agnostic. However multiple languages\n",
            "        will not be aligned.\n",
            "\n",
            "        Using the universal sentence encoder options will be much faster since\n",
            "        those are pre-trained and efficient models. The universal sentence\n",
            "        encoder options are suggested for smaller data sets. They are also\n",
            "        good options for large data sets that are in English or in languages\n",
            "        covered by the multilingual model. It is also suggested for data sets\n",
            "        that are multilingual.\n",
            "\n",
            "        For more information on universal-sentence-encoder options visit:\n",
            "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
            "\n",
            "        The SBERT pre-trained sentence transformer options are\n",
            "        distiluse-base-multilingual-cased,\n",
            "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
            "\n",
            "        The distiluse-base-multilingual-cased and\n",
            "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
            "        datasets and languages that are not\n",
            "        covered by the multilingual universal sentence encoder. The\n",
            "        transformer is significantly slower than the universal sentence\n",
            "        encoder options(except for the large options).\n",
            "\n",
            "        For more information on SBERT options visit:\n",
            "        https://www.sbert.net/docs/pretrained_models.html\n",
            "\n",
            "        If passing a callable embedding_model note that it will not be saved\n",
            "        when saving a top2vec model. After loading such a saved top2vec model\n",
            "        the set_embedding_model method will need to be called and the same\n",
            "        embedding_model callable used during training must be passed to it.\n",
            "\n",
            "    embedding_model_path: string (Optional)\n",
            "        Pre-trained embedding models will be downloaded automatically by\n",
            "        default. However they can also be uploaded from a file that is in the\n",
            "        location of embedding_model_path.\n",
            "\n",
            "        Warning: the model at embedding_model_path must match the\n",
            "        embedding_model parameter type.\n",
            "\n",
            "    embedding_batch_size: int (default=32)\n",
            "        Batch size for documents being embedded.\n",
            "\n",
            "    split_documents: bool (default False)\n",
            "        If set to True, documents will be split into parts before embedding.\n",
            "        After embedding the multiple document part embeddings will be averaged\n",
            "        to create a single embedding per document. This is useful when documents\n",
            "        are very large or when the embedding model has a token limit.\n",
            "\n",
            "        Document chunking or a senticizer can be used for document splitting.\n",
            "\n",
            "    document_chunker: string or callable (default 'sequential')\n",
            "        This will break the document into chunks. The valid string options are:\n",
            "\n",
            "            * sequential\n",
            "            * random\n",
            "\n",
            "        The sequential chunker will split the document into chunks of specified\n",
            "        length and ratio of overlap. This is the recommended method.\n",
            "\n",
            "        The random chunking option will take random chunks of specified length\n",
            "        from the document. These can overlap and should be thought of as\n",
            "        sampling chunks with replacement from the document.\n",
            "\n",
            "        If a callable is passed it must take as input a list of tokens of\n",
            "        a document and return a list of strings representing the resulting\n",
            "        document chunks.\n",
            "\n",
            "        Only one of document_chunker or sentincizer should be used.\n",
            "\n",
            "    chunk_length: int (default 100)\n",
            "        The number of tokens per document chunk if using the document chunker\n",
            "        string options.\n",
            "\n",
            "    max_num_chunks: int (Optional)\n",
            "        The maximum number of chunks generated per document if using the\n",
            "        document chunker string options.\n",
            "\n",
            "    chunk_overlap_ratio: float (default 0.5)\n",
            "        Only applies to the 'sequential' document chunker.\n",
            "\n",
            "        Fraction of overlapping tokens between sequential chunks. A value of\n",
            "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
            "        previous chunk.\n",
            "\n",
            "    chunk_len_coverage_ratio: float (default 1.0)\n",
            "        Only applies to the 'random' document chunker option.\n",
            "\n",
            "        Proportion of token length that will be covered by chunks. Default\n",
            "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
            "        the document. This does not mean all tokens will be covered since\n",
            "        chunks can be overlapping.\n",
            "\n",
            "    sentencizer: callable (Optional)\n",
            "        A sentincizer callable can be passed. The input should be a string\n",
            "        representing the document and the output should be a list of strings\n",
            "        representing the document sentence chunks.\n",
            "\n",
            "        Only one of document_chunker or sentincizer should be used.\n",
            "\n",
            "    speed: string (Optional, default 'learn')\n",
            "\n",
            "        This parameter is only used when using doc2vec as embedding_model.\n",
            "\n",
            "        It will determine how fast the model takes to train. The\n",
            "        fast-learn option is the fastest and will generate the lowest quality\n",
            "        vectors. The learn option will learn better quality vectors but take\n",
            "        a longer time to train. The deep-learn option will learn the best\n",
            "        quality vectors but will take significant time to train. The valid\n",
            "        string speed options are:\n",
            "        \n",
            "            * fast-learn\n",
            "            * learn\n",
            "            * deep-learn\n",
            "\n",
            "    use_corpus_file: bool (Optional, default False)\n",
            "\n",
            "        This parameter is only used when using doc2vec as embedding_model.\n",
            "\n",
            "        Setting use_corpus_file to True can sometimes provide speedup for\n",
            "        large datasets when multiple worker threads are available. Documents\n",
            "        are still passed to the model as a list of str, the model will create\n",
            "        a temporary corpus file for training.\n",
            "\n",
            "    document_ids: List of str, int (Optional)\n",
            "        A unique value per document that will be used for referring to\n",
            "        documents in search results. If ids are not given to the model, the\n",
            "        index of each document in the original corpus will become the id.\n",
            "\n",
            "    keep_documents: bool (Optional, default True)\n",
            "        If set to False documents will only be used for training and not saved\n",
            "        as part of the model. This will reduce model size. When using search\n",
            "        functions only document ids will be returned, not the actual\n",
            "        documents.\n",
            "\n",
            "    workers: int (Optional)\n",
            "        The amount of worker threads to be used in training the model. Larger\n",
            "        amount will lead to faster training.\n",
            "    \n",
            "    tokenizer: callable (Optional, default None)\n",
            "        Override the default tokenization method. If None then\n",
            "        gensim.utils.simple_preprocess will be used.\n",
            "\n",
            "        Tokenizer must take a document and return a list of tokens.\n",
            "\n",
            "    use_embedding_model_tokenizer: bool (Optional, default True)\n",
            "        If using an embedding model other than doc2vec, use the model's\n",
            "        tokenizer for document embedding. If set to True the tokenizer, either\n",
            "        default or passed callable will be used to tokenize the text to\n",
            "        extract the vocabulary for word embedding.\n",
            "\n",
            "    umap_args: dict (Optional, default None)\n",
            "        Pass custom arguments to UMAP.\n",
            "\n",
            "    gpu_umap: bool (default False)\n",
            "        If True umap will use the rapidsai cuml library to perform the\n",
            "        dimensionality reduction. This will lead to a significant speedup\n",
            "        in the computation time during model createion. To install rapidsai\n",
            "        cuml follow the instructions here: https://docs.rapids.ai/install\n",
            "\n",
            "    hdbscan_args: dict (Optional, default None)\n",
            "        Pass custom arguments to HDBSCAN.\n",
            "\n",
            "    gpu_hdbscan: bool (default False)\n",
            "        If True hdbscan will use the rapidsai cuml library to perform the\n",
            "        clustering. This will lead to a significant speedup in the computation\n",
            "        time during model creation. To install rapidsai cuml follow the\n",
            "        instructions here: https://docs.rapids.ai/install\n",
            "\n",
            "    index_topics: bool (Optional, default False)\n",
            "        If True, the topic vectors will be indexed using hnswlib. This will\n",
            "        significantly speed up finding topics during model creation for\n",
            "        very large datasets.\n",
            "    \n",
            "    verbose: bool (Optional, default True)\n",
            "        Whether to print status data during training.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TRAIN MODEL"
      ],
      "metadata": {
        "id": "7r-pXhYmVSnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_min_20_ngram_large = Top2Vec(docs,min_count=20, ngram_vocab=True,embedding_model='universal-sentence-encoder-large')"
      ],
      "metadata": {
        "id": "N0eUzNcIVT-j",
        "outputId": "3feb44fd-a1a7-42d9-9f8a-3b03fd0e8966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-05 17:43:05,075 - top2vec - INFO - Pre-processing documents for training\n",
            "INFO:top2vec:Pre-processing documents for training\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "2024-04-05 17:43:10,867 - top2vec - INFO - Downloading universal-sentence-encoder-large model\n",
            "INFO:top2vec:Downloading universal-sentence-encoder-large model\n"
          ]
        }
      ]
    }
  ]
}