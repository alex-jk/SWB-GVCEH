{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYWBGMWbtT6nv23jZly6a+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/SWB-GVCEH/blob/main/reddit_scraper/Unique_Search_Terms_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "chaCMp772BF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754970ff-ed4d-434d-c829-cc58d8590a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#!rm -rf /content/SWB-GVCEH  # Removes the entire SWB-GVCEH directory\n",
        "#!git clone https://github.com/alex-jk/SWB-GVCEH.git  # Clones the repository again\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8M56xzLexiK",
        "outputId": "c5a710db-9635-4794-8dd6-b690b1606bac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n",
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: aiofiles<1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.8.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.8.6)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "# Set your PAT here. Make sure to clear this cell's output or delete the PAT after setting the environment variable.\n",
        "os.environ['GITHUB_PAT'] = 'xxx'  # Replace 'your_pat_here' with your actual PAT.\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import re\n",
        "import asyncpraw.models\n",
        "import time\n",
        "from collections import deque\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "MtPcWLd66zOA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd SWB-GVCEH\n",
        "# !ls"
      ],
      "metadata": {
        "id": "SaFq-yQr2I3L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"alex-jk\"\n",
        "!git config --global user.email \"alex.joukova@gmail.com\""
      ],
      "metadata": {
        "id": "R6v9Syii52ZK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the Reddit client\n",
        "user_agent=\"my_scraper:v1.0 (by /u/neuro-psych-amateur)\"\n",
        "client_id='syz99Sr36y8ZEAwMYMqa1A'\n",
        "client_secret='pWjCafggxLSgDkjUyospIJn_2w1oww'\n",
        "\n",
        "reddit = asyncpraw.Reddit(client_id=client_id,\n",
        "                     client_secret=client_secret,  # Insert your client_secret here\n",
        "                     user_agent=user_agent)  # This identifies your script to Reddit"
      ],
      "metadata": {
        "id": "63vr4R_MeZyX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddits_list = ['VictoriaBC', 'gulfislands'] # ['Sooke', 'VictoriaBC', 'gulfislands']"
      ],
      "metadata": {
        "id": "P8NkydWWb58H"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file names\n",
        "file_names = [\n",
        "    'GVCEH-2022-07-25-tweet-cleaned.csv',\n",
        "    'GVCEH-2022-07-18-tweet-cleaned.csv',\n",
        "    'GVCEH-2022-07-13-tweet-cleaned.csv',\n",
        "    'GVCEH-2022-07-12-tweet-cleaned.csv'\n",
        "]\n",
        "\n",
        "# Base URL for raw files in the GitHub repository\n",
        "base_url = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/data/cleaned/'\n",
        "\n",
        "# Initialize a list to collect the DataFrames\n",
        "dfs = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    # Construct the full URL for the current file\n",
        "    file_url = base_url + file_name\n",
        "    # Read the CSV file\n",
        "    current_df = pd.read_csv(file_url)\n",
        "    # Append the DataFrame to the list\n",
        "    dfs.append(current_df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Remove duplicates\n",
        "combined_df = combined_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Displaying the first few rows of the DataFrame\n",
        "print(combined_df.shape)\n",
        "print(combined_df.columns)\n",
        "print(combined_df.head())\n",
        "#print(GVCEH_2022_07_25_tweet_cleaned['search_keywords'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIq7A2282urd",
        "outputId": "6ce8af06-e519-4c8f-b516-083f51fe8338"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6268, 17)\n",
            "Index(['Unnamed: 0', 'text', 'scrape_time', 'tweet_id', 'created_at',\n",
            "       'reply_count', 'quote_count', 'like_count', 'retweet_count',\n",
            "       'geo_full_name', 'geo_id', 'geo_bbox', 'tweet_coordinate', 'username',\n",
            "       'num_followers', 'search_keywords', 'search_neighbourhood'],\n",
            "      dtype='object')\n",
            "   Unnamed: 0                                               text  \\\n",
            "0           4  “The black character entered mainstream postwa...   \n",
            "1           5  “The black character entered mainstream postwa...   \n",
            "2           8  @dizzyreader1 @SheldonTheCat07 I do think they...   \n",
            "3           9  @blue_heathen @InDecades @newsmax Of so, speci...   \n",
            "4          10  \"Come sleep, o sleep, the certain knot of peac...   \n",
            "\n",
            "                  scrape_time             tweet_id                 created_at  \\\n",
            "0  2022-07-25 10:50:38.621962  1550946480596426753  2022-07-23 20:50:30+00:00   \n",
            "1  2022-07-25 10:50:38.621962  1550938802469879811  2022-07-23 20:20:00+00:00   \n",
            "2  2022-07-25 10:50:40.099375  1551118966273212417  2022-07-24 08:15:54+00:00   \n",
            "3  2022-07-25 10:50:40.099375  1550913818385125377  2022-07-23 18:40:43+00:00   \n",
            "4  2022-07-25 10:50:40.099375  1550888278735278083  2022-07-23 16:59:14+00:00   \n",
            "\n",
            "   reply_count  quote_count  like_count  retweet_count geo_full_name geo_id  \\\n",
            "0            0            0           0              0           NaN    NaN   \n",
            "1            0            2          19              7           NaN    NaN   \n",
            "2            0            0           4              1           NaN    NaN   \n",
            "3            4            0           0              0           NaN    NaN   \n",
            "4            0            0           0              0           NaN    NaN   \n",
            "\n",
            "  geo_bbox tweet_coordinate         username  num_followers  \\\n",
            "0      NaN              NaN          ojalart           7326   \n",
            "1      NaN              NaN      parisreview        1006255   \n",
            "2      NaN              NaN  Mimithe20536023            226   \n",
            "3      NaN              NaN  TimeTravlPundit             89   \n",
            "4      NaN              NaN         SarbaniC            309   \n",
            "\n",
            "                                     search_keywords  \\\n",
            "0  (colwood OR sidney OR cook street village OR r...   \n",
            "1  (colwood OR sidney OR cook street village OR r...   \n",
            "2  (colwood OR sidney OR cook street village OR r...   \n",
            "3  (colwood OR sidney OR cook street village OR r...   \n",
            "4  (colwood OR sidney OR cook street village OR r...   \n",
            "\n",
            "                                search_neighbourhood  \n",
            "0  colwood OR sidney OR cook street village OR ro...  \n",
            "1  colwood OR sidney OR cook street village OR ro...  \n",
            "2  colwood OR sidney OR cook street village OR ro...  \n",
            "3  colwood OR sidney OR cook street village OR ro...  \n",
            "4  colwood OR sidney OR cook street village OR ro...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_search_terms(text):\n",
        "    # Remove any leading/trailing whitespace and the \"lang:en -is:retweet\" part\n",
        "    text = text.strip().rsplit(' lang:', 1)[0]\n",
        "\n",
        "    # Split the text on ' OR ' after removing brackets\n",
        "    terms = re.split(r'\\s+OR\\s+', re.sub(r'[()]', '', text))\n",
        "\n",
        "    return terms\n",
        "\n",
        "extracted_terms  = combined_df['search_keywords'].apply(extract_search_terms)\n",
        "print(extracted_terms .head())\n",
        "print(extracted_terms[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICwte61H3Vp2",
        "outputId": "9e19d3db-c338-4f9f-8b02-80efb0bcb5ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [colwood, sidney, cook street village, rockhei...\n",
            "1    [colwood, sidney, cook street village, rockhei...\n",
            "2    [colwood, sidney, cook street village, rockhei...\n",
            "3    [colwood, sidney, cook street village, rockhei...\n",
            "4    [colwood, sidney, cook street village, rockhei...\n",
            "Name: search_keywords, dtype: object\n",
            "['colwood', 'sidney', 'cook street village', 'rockheights', 'hollywood park', 'songhees walkway', 'lekwungen', \"sc'ianew aceh\", 'city of langford', 'greater victoria housing society', 'peers victoria resources society', 'substance uvic', 'the victoria foundation', 'victoria chamber', 'workbc', 'people with lived experience', 'service provider', 'front line', 'poverty', 'tent', 'affordable', 'alcoholic', 'social problem']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Combine all lists into one\n",
        "all_terms = [term for sublist in extracted_terms for term in sublist]\n",
        "\n",
        "# Step 2: Extract unique terms\n",
        "unique_terms = list(set(term.replace('from:', '') for term in all_terms))\n",
        "unique_terms.sort()\n",
        "\n",
        "# Print or inspect the unique terms\n",
        "print(f\"Number of search terms: {len(unique_terms)}\")\n",
        "print(unique_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Npps4E4PHK",
        "outputId": "43b6de10-1b10-4d3a-a2af-dd016942f51d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of search terms: 587\n",
            "['#YYJ boys \"and\" girls club south vanouver island', '1upparents anawim house', '1upparents avi', '1upparents bc housing', '1upparents boys \"and\" girls club south vanouver island', '900-block pandora avenue', 'Greater Victoria Coalition to End Homelessness bc housing', 'Greater Victoria aceh', 'Greater Victoria aids vancouver island', 'Greater Victoria aryze developments', 'Greater Victoria avi', 'Greater Victoria bc housing', 'Greater Victoria beacon community services', 'Greater Victoria boys \"and\" girls club south vanouver island', 'Victoria B.C. aids vancouver island', 'Victoria B.C. anawim house', 'Victoria B.C. aryze developments', 'Victoria B.C. avi', 'Victoria B.C. bc housing', 'Victoria B.C. boys \"and\" girls club south vanouver island', 'Victoria aceh', 'Victoria aids vancouver island', 'Victoria anawim house', 'Victoria aryze developments', 'Victoria avi', 'Victoria bc housing', 'Victoria beacon community services', 'Victoria boys \"and\" girls club south vanouver island', 'Victoria burnside gorge neighbourhood association', 'Victoria the aboriginal coalition to end homelessness', 'VictoriaBC aceh', 'VictoriaBC aids vancouver island', 'VictoriaBC anawim house', 'VictoriaBC avi', 'VictoriaBC bc housing', 'VictoriaBC boys \"and\" girls club south vanouver island', 'VictoriaBC burnside gorge neighbourhood association', 'VictoriaBC the aboriginal coalition to end homelessness', 'YYJ aceh', 'YYJ aids vancouver island', 'YYJ anawim house', 'YYJ aryze developments', 'YYJ avi', 'YYJ bc housing', 'YYJ beacon community services', 'YYJ boys \"and\" girls club south vanouver island', 'YYJ burnside gorge neighbourhood association', 'YYJ the aboriginal coalition to end homelessness', 'acehsociety', 'adam_stirling aceh', 'adam_stirling aids vancouver island', 'adam_stirling anawim house', 'adam_stirling aryze developments', 'adam_stirling avi', 'adam_stirling bc housing', 'adam_stirling beacon community services', 'adam_stirling boys \"and\" girls club south vanouver island', 'adam_stirling burnside gorge neighbourhood association', 'adampolsen', 'addict', 'addicted', 'adriandix', 'affordable', 'affordable housing', 'alanrycroft anawim house', 'alanrycroft aryze developments', 'alanrycroft avi', 'alanrycroft bc housing', 'alanrycroft boys \"and\" girls club south vanouver island', 'alanrycroft burnside gorge neighbourhood association', 'alcoholic', 'anawimhouse aids vancouver island', 'anawimhouse bc housing', 'anawimhouse boys \"and\" girls club south vanouver island', 'avivanisle', 'barbdesjardins', 'barclaywallace', 'bc_housing', 'beacon hill park', 'beaconhillfolks', 'beaconsave', 'berniepauly', 'brucealready avi', 'buckleygkml', 'burnside-gorge', 'burnsidegorge aceh', 'burnsidegorge aids vancouver island', 'burnsidegorge aryze developments', 'burnsidegorge avi', 'burnsidegorge beacon community services', 'burnsidegorge burnside gorge neighbourhood association', 'camp', 'camper', 'camping', 'carolinaibarra', 'cbcnews aceh', 'cbcnews anawim house', 'cbcnews aryze developments', 'cbcnews avi', 'cbcnews bc housing', 'cbcnews beacon community services', 'cbcnews boys \"and\" girls club south vanouver island', 'cbcnews burnside gorge neighbourhood association', 'cbcnews the aboriginal coalition to end homelessness', 'cecelia ravine park', 'central park', 'cfax1070', 'charlesbodi', 'chartj88', 'chek_news', 'chiefmanak', 'city of colwood', 'city of langford', 'city of victoria', 'cityofvictoria', 'coast salish', 'colinplant2018', 'collude', 'collusion', 'colwood', 'community plan to end homelessness', 'complex care', 'cook street village', 'cool aid society', 'coordinated access \"and\" assessment', 'coreyranger', 'coryresilient', 'councllrntaylor', 'crd_bc', 'crime', 'cspc_victoria avi', 'cspc_victoria bc housing', 'cspc_victoria boys \"and\" girls club south vanouver island', 'cspoliceservice', 'ctess1', 'ctvnewsvi', 'darylbassett3', 'dave_eby', 'davidhscreech', 'district of north saanich', 'district of saanich', 'downtown victoria business association', 'drcvictoria', 'drug use', 'drugs', 'dvba', 'elizabethmay', 'emergency housing', 'encampment', 'eric_doherty', 'esquimalt', 'esquimaltbc', 'esquimaltnation', 'evict', 'evicted', 'eviction', 'existenceprojct', 'extremeoutreach', 'fairfield-gonzales', 'fairfield_comm', 'fernwood', 'fernwood community association', 'fernwoodfca', 'firstmetvic', 'foul bay', 'foundry victoria', 'foundryvictoria', 'front line', 'front line worker', 'frontline', 'frontline worker', 'galloping goose', 'goldstreamnews', 'gonzales', 'gonzales park', 'gracealore aceh', 'gracealore aids vancouver island', 'gracealore anawim house', 'gracealore aryze developments', 'gracealore avi', 'gracealore bc housing', 'gracealore beacon community services', 'gracealore boys \"and\" girls club south vanouver island', 'gracealore burnside gorge neighbourhood association', 'gracealore the aboriginal coalition to end homelessness', 'greater victoria acting together', 'greater victoria coalition to end homelessness', 'greater victoria housing society', 'habitat for humanity victoria', 'habitatvictoria', 'harm reduction', 'harris green', 'hattiecosta1', 'healthcare', 'hilarylmarks', 'hillside-quadra', 'hollywood park', 'homeforhope', 'homeless', 'homelessness', 'homelessness services association of bc', 'houseanawim', 'housesooke', 'housing', 'ilfp_victoria beacon community services', 'irving park', 'island health', 'islandcommha', 'jamesbaycp', 'jeremyloveday', 'jfatkey avi', 'jfatkey bc housing', 'johnhoward_can', 'juan de fuca', 'katstinson', 'kevinalbersbc aceh', 'kevinalbersbc aids vancouver island', 'kevinalbersbc avi', 'kevinalbersbc bc housing', 'kevinalbersbc boys \"and\" girls club south vanouver island', 'kristaloughton', 'kwakwaka’wakw', 'langford', 'laurel_bc', 'lekwungen', 'literacy victoria', 'lived experience', 'lochside', 'louise_hartland aceh', 'louise_hartland anawim house', 'louise_hartland aryze developments', 'louise_hartland avi', 'louise_hartland bc housing', 'louise_hartland burnside gorge neighbourhood association', 'low income', 'low-income', 'majatait', 'makola housing society', 'makolahousing', 'malahat aceh', 'malahat aids vancouver island', 'malahat anawim house', 'malahat aryze developments', 'malahat avi', 'malahat bc housing', 'malahat beacon community services', 'malahat boys \"and\" girls club south vanouver island', 'malahat burnside gorge neighbourhood association', 'malahat the aboriginal coalition to end homelessness', 'marikaalbert', 'mental health society of greater victoria', 'metchosin', 'mhrpsouthisland', 'mhsvictoria anawim house', 'mhsvictoria avi', 'mhsvictoria boys \"and\" girls club south vanouver island', 'millstream', 'murdochoakbay aceh', 'murdochoakbay anawim house', 'murdochoakbay aryze developments', 'murdochoakbay avi', 'murdochoakbay bc housing', 'murdochoakbay boys \"and\" girls club south vanouver island', 'murdochoakbay burnside gorge neighbourhood association', 'murdochoakbay the aboriginal coalition to end homelessness', 'murray_langdon', 'mustardseedvic anawim house', 'mustardseedvic avi', 'mustardseedvic bc housing', 'mustardseedvic burnside gorge neighbourhood association', 'mydvba', 'narcotics', 'need_2', 'nicolechaland', 'ninethreeseven', 'north park', 'nuu-chah-nulth', 'nuu-chah-nulth aceh', 'nuu-chah-nulth aids vancouver island', 'nuu-chah-nulth anawim house', 'nuu-chah-nulth aryze developments', 'nuu-chah-nulth avi', 'nuu-chah-nulth bc housing', 'nuu-chah-nulth beacon community services', 'nuu-chah-nulth boys \"and\" girls club south vanouver island', 'nuu-chah-nulth burnside gorge neighbourhood association', 'nuu-chah-nulth the aboriginal coalition to end homelessness', 'oak bay', 'oakbaynews', 'oaklands', 'oaklands park', 'ops', 'our place society', 'ourplacesociety', 'outreachsolid', 'overdose', 'overdosed', 'pacheedaht aceh', 'pacheedaht aids vancouver island', 'pacheedaht anawim house', 'pacheedaht aryze developments', 'pacheedaht avi', 'pacheedaht bc housing', 'pacheedaht beacon community services', 'pacheedaht boys \"and\" girls club south vanouver island', 'pacheedaht burnside gorge neighbourhood association', 'pacheedaht the aboriginal coalition to end homelessness', 'pacifica housing', 'pacificahousing', 'pacochran', 'pandora avenue', 'pauquachin aceh', 'pauquachin aids vancouver island', 'pauquachin anawim house', 'pauquachin aryze developments', 'pauquachin avi', 'pauquachin bc housing', 'pauquachin beacon community services', 'pauquachin boys \"and\" girls club south vanouver island', 'pauquachin burnside gorge neighbourhood association', 'peer housing support', 'peer housing support program', 'peers victoria resources society', 'peersvictoria', 'people with lived experience', 'person experiencing homelessness', 'photowarrior aceh', 'photowarrior aids vancouver island', 'photowarrior anawim house', 'photowarrior avi', 'photowarrior bc housing', 'photowarrior beacon community services', 'photowarrior boys \"and\" girls club south vanouver island', 'photowarrior the aboriginal coalition to end homelessness', 'pit', 'pit count', 'point ellice park', 'point in time', 'poor', 'poverty', 'povertypimps', 'quadra', 'quadra village', 'quadra village community centre', 'quadravillage', 'r_garrison', 'reaching home', 'red cedar café', 'rentsmart', 'rentsmartedu', 'restorative justice victoria', 'rock bay', 'rockheights', 'rockland', 'rotary club of victoria', 'royal athletic park', 'saanich', 'saanichnews', 'saanichpolice', 'saanichton', 'safe supply', 'safer victoria', 'safervic', 'safervic aceh', 'safervic bc housing', 'salarmyvicarc', 'salt spring island', 'saltspringx', 'salvation army victoria', 'sarahpottsvic aceh', 'sarahpottsvic aids vancouver island', 'sarahpottsvic anawim house', 'sarahpottsvic aryze developments', 'sarahpottsvic avi', 'sarahpottsvic bc housing', 'sarahpottsvic beacon community services', 'sarahpottsvic boys \"and\" girls club south vanouver island', 'sarahpottsvic burnside gorge neighbourhood association', 'sarahpottsvic the aboriginal coalition to end homelessness', \"sc'ianew aceh\", \"sc'ianew aids vancouver island\", \"sc'ianew anawim house\", \"sc'ianew aryze developments\", \"sc'ianew avi\", \"sc'ianew bc housing\", \"sc'ianew beacon community services\", 'sc\\'ianew boys \"and\" girls club south vanouver island', \"sc'ianew burnside gorge neighbourhood association\", \"sc'ianew the aboriginal coalition to end homelessness\", 'self_govern4us', 'selkirk green', 'selkirk trestle', 'service provider', 'shelliegudgeon', 'shelter', 'sidney', 'sjavicbc', 'soap4hopeyyj', 'social housing', 'social problem', 'social structure', 'solid outreach', 'songhees', 'songhees walkway', 'songheeschief', 'sooke', 'sooke homelessness coalition', 'sookecoalition', 'sookenews', 'south_island_c', 'spaynebc', 'spd_community', 'stadacona park', 'stephen_andrew', 'stolen', 'subsidized housing', 'substance use', 'substance uvic', 'substanceuvic', 'svdpvi', \"t'sou-ke aceh\", \"t'sou-ke aids vancouver island\", \"t'sou-ke anawim house\", \"t'sou-ke aryze developments\", \"t'sou-ke avi\", \"t'sou-ke bc housing\", \"t'sou-ke beacon community services\", 't\\'sou-ke boys \"and\" girls club south vanouver island', \"t'sou-ke burnside gorge neighbourhood association\", 'tagvictoriabc aceh', 'tagvictoriabc aids vancouver island', 'tagvictoriabc avi', 'tagvictoriabc bc housing', 'tagvictoriabc beacon community services', 'tagvictoriabc boys \"and\" girls club south vanouver island', 'tagvictoriabc burnside gorge neighbourhood association', 'tagvictoriabc the aboriginal coalition to end homelessness', 'talktoaryze', 'temporary shelter', 'tenant action group victoria', 'tent', 'the backpack project', 'the cridge centre for the family', 'the downtown victoria business association', 'the existence project', 'the gorge', 'the homeless ideas podcast', 'the john howard society of victoria', 'the mustard seed', 'the victoria foundation', 'the victoria real estate board', 'thebackpackpro1', 'thecridgecentre', 'theft', 'thief', 'timescolonist', 'togethervic', 'topaz park', 'tourismvi', 'town of view royal', 'township of esquimalt', 'tsartlip aceh', 'tsartlip aids vancouver island', 'tsartlip anawim house', 'tsartlip aryze developments', 'tsartlip avi', 'tsartlip bc housing', 'tsartlip beacon community services', 'tsartlip boys \"and\" girls club south vanouver island', 'tsartlip burnside gorge neighbourhood association', 'tsawout aceh', 'tsawout aids vancouver island', 'tsawout anawim house', 'tsawout aryze developments', 'tsawout avi', 'tsawout bc housing', 'tsawout beacon community services', 'tsawout boys \"and\" girls club south vanouver island', 'tsawout burnside gorge neighbourhood association', 'tsawout the aboriginal coalition to end homelessness', 'tseycum aceh', 'tseycum aids vancouver island', 'tseycum anawim house', 'tseycum aryze developments', 'tseycum avi', 'tseycum bc housing', 'tseycum beacon community services', 'tseycum boys \"and\" girls club south vanouver island', 'tseycum burnside gorge neighbourhood association', 'umbrella society', 'umbrellasociety', 'unhoused', 'united way southern vancouver island', 'unitedatoakbay', 'uplands', 'uvic aids vancouver island', 'uvic anawim house', 'uvic aryze developments', 'uvic avi', 'uvic beacon community services', 'vancouver island mental health society', 'vandupeople', 'vanislandhealth', 'varcsvictoria', 'vfamcourt aceh', 'vfamcourt avi', 'vfamcourt burnside gorge neighbourhood association', 'vibrant victoria', 'vic west', 'vic west park', 'viccoolaid', 'vicfoundation', 'vicpdcanada', 'vicplacemaking aceh', 'vicplacemaking aids vancouver island', 'vicplacemaking anawim house', 'vicplacemaking aryze developments', 'vicplacemaking avi', 'vicplacemaking bc housing', 'vicplacemaking beacon community services', 'vicplacemaking burnside gorge neighbourhood association', 'vicplacemaking the aboriginal coalition to end homelessness', 'victoria', 'victoria brain injury society', 'victoria chamber', 'victoria family court', 'victoria harbour cats', 'victoria native friendship centre', 'victoria placemaking', 'victoria ready', 'victoria real estate board', 'victoria sexual assault centre', 'victoria tenant action group', 'victoria west', 'victoria women in need', 'victoriabuzzes', 'victoriadra aceh', 'victoriadra bc housing', 'victoriadra the aboriginal coalition to end homelessness', 'victorianews aceh', 'victorianews aids vancouver island', 'victorianews anawim house', 'victorianews aryze developments', 'victorianews avi', 'victorianews bc housing', 'victorianews beacon community services', 'victorianews boys \"and\" girls club south vanouver island', 'victorianews burnside gorge neighbourhood association', 'victorianews the aboriginal coalition to end homelessness', 'victoriasandy', 'victoriavisitor aceh', 'victoriavisitor aids vancouver island', 'victoriavisitor burnside gorge neighbourhood association', 'victoriavisitor the aboriginal coalition to end homelessness', 'victoriaworkbc', 'victoriawth anawim house', 'vicyouthcouncil', 'view royal', 'violence', 'vreb', 'vreb aceh', 'vreb aids vancouver island', 'vreb anawim house', 'vreb avi', 'vreb bc housing', 'vreb beacon community services', 'vreb boys \"and\" girls club south vanouver island', 'wearenorthpark', 'west shore', 'workbc', 'workingupstream', 'wschamber1', 'wsáneć aceh', 'wsáneć aids vancouver island', 'wsáneć anawim house', 'wsáneć aryze developments', 'wsáneć avi', 'wsáneć bc housing', 'wsáneć beacon community services', 'wsáneć boys \"and\" girls club south vanouver island', 'wsáneć burnside gorge neighbourhood association', 'wsáneć the aboriginal coalition to end homelessness', 'youngparentssup', 'yyj_housing', 'yyjpolitics', 'zacdevries avi', 'zacdevries burnside gorge neighbourhood association']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collect data from specific subreddits"
      ],
      "metadata": {
        "id": "sSRCCcXjb3j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# async def fetch_data(reddit, subreddit_names, limit_num=5000):\n",
        "#     titles = []\n",
        "#     texts = []\n",
        "#     user_ids = []\n",
        "#     comments = []\n",
        "#     subreddits = []\n",
        "\n",
        "#     for subreddit_name in subreddit_names:\n",
        "#         print(f\"\\nFetching data from subreddit: {subreddit_name}\")\n",
        "#         subreddit = await reddit.subreddit(subreddit_name)\n",
        "\n",
        "#         submissions = []\n",
        "#         async for submission in subreddit.hot(limit=limit_num):  # You can change .hot to .new or .top if needed\n",
        "#             submissions.append(submission)\n",
        "#         print(f\"Found {len(submissions)} submissions in subreddit '{subreddit_name}'\")\n",
        "\n",
        "#         for submission in submissions:\n",
        "#             print(f\"Processing submission: {submission.id}\")\n",
        "#             await asyncio.sleep(1)  # Introduce a delay to respect rate limits\n",
        "#             await submission.load()\n",
        "#             comment_queue = submission.comments[:]\n",
        "\n",
        "#             comment_count = 0\n",
        "#             while comment_queue:\n",
        "#                 comment = comment_queue.pop(0)\n",
        "#                 if isinstance(comment, asyncpraw.models.Comment):\n",
        "#                     titles.append(submission.title)\n",
        "#                     texts.append(submission.selftext)\n",
        "#                     user_ids.append(submission.author.name if submission.author else None)\n",
        "#                     comments.append(comment.body)\n",
        "#                     subreddits.append(subreddit_name)\n",
        "#                     comment_count += 1\n",
        "#                 elif isinstance(comment, asyncpraw.models.MoreComments):\n",
        "#                     more_comments = await comment.comments()\n",
        "#                     comment_queue.extend(more_comments)\n",
        "\n",
        "#             print(f\"Appended {comment_count} comments for submission {submission.id}\")\n",
        "\n",
        "#     df = pd.DataFrame({\n",
        "#         'Subreddit': subreddits,\n",
        "#         'Title': titles,\n",
        "#         'Text': texts,\n",
        "#         'User ID': user_ids,\n",
        "#         'Comment': comments\n",
        "#     })\n",
        "\n",
        "#     return df"
      ],
      "metadata": {
        "id": "brNNjcsef2M2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# async def main():\n",
        "#     subreddits = ['Sooke', 'VictoriaBC', 'gulfislands']\n",
        "#     df = await fetch_data(reddit, subreddits)\n",
        "\n",
        "#     await reddit.close()\n",
        "#     return df\n",
        "\n",
        "# # Run the main function and get the dataframe\n",
        "# dataframe = await main()"
      ],
      "metadata": {
        "id": "rDwjWn4qiLMW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dataframe.shape)\n",
        "# print(dataframe.columns)\n",
        "# print(dataframe.head(50))\n",
        "\n",
        "# dataframe.to_csv('reddit_data.csv', index=False)"
      ],
      "metadata": {
        "id": "c8s0sUvbs2Er"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define search terms"
      ],
      "metadata": {
        "id": "oLiOIQXrtRL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls /content/SWB-GVCEH/data"
      ],
      "metadata": {
        "id": "vssAOPE4tTnp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "3jQ4vo8xw_VV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls\n",
        "# !mv Reddit_Search_Terms.csv /content/SWB-GVCEH/data/Reddit_Search_Terms.csv"
      ],
      "metadata": {
        "id": "KieLXYiWxFSL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your CSV file\n",
        "# file_path = '/content/SWB-GVCEH/data/Reddit_Search_Terms.csv'\n",
        "file_path = 'https://raw.githubusercontent.com/alex-jk/SWB-GVCEH/main/data/Reddit_Search_Terms.csv'\n",
        "# Load the CSV file into a DataFrame\n",
        "reddit_search_terms_df = pd.read_csv(file_path)\n",
        "# Display the first few rows of the DataFrame\n",
        "print(reddit_search_terms_df.head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM2_zjIEyUz_",
        "outputId": "67b650d1-acc4-4f68-bb68-984a8b9ec698"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 search_term\n",
            "0                              #PovertyPimps\n",
            "1   Aboriginal Coalition to End Homelessness\n",
            "2                                       ACEH\n",
            "3                                     Addict\n",
            "4                                   Addicted\n",
            "5                                 Affordable\n",
            "6                         Affordable Housing\n",
            "7                        Affordable housing \n",
            "8                                 Alcoholic \n",
            "9                                     Anawim\n",
            "10                                BC Housing\n",
            "11                                      Camp\n",
            "12                                    Camper\n",
            "13                                   Camping\n",
            "14                                   Collude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pwd\n",
        "# !ls\n",
        "# %cd /content\n",
        "# !ls\n",
        "# %cd /content/SWB-GVCEH"
      ],
      "metadata": {
        "id": "SLCP5Ojt1NJO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from getpass import getpass\n",
        "# pat = getpass('Enter your PAT: ')"
      ],
      "metadata": {
        "id": "poVlJX5K5vgN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git remote set-url origin https://alex-jk:{pat}@github.com/alex-jk/SWB-GVCEH.git"
      ],
      "metadata": {
        "id": "-1s8Bzg75-qh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git add '/content/SWB-GVCEH/data/Reddit_Search_Terms.csv'\n",
        "# !git commit -m \"Added Reddit Search Terms list csv file\""
      ],
      "metadata": {
        "id": "nbDeKRzXyeV1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git status\n",
        "# !git push origin main"
      ],
      "metadata": {
        "id": "Q4Zuzz7h0hDz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_terms = reddit_search_terms_df['search_term'].tolist()\n",
        "search_terms = list( set( search_terms + unique_terms ))\n",
        "search_terms.sort()\n",
        "\n",
        "print(len(search_terms))\n",
        "print(search_terms)  # Print to verify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4ZawADN0Ljx",
        "outputId": "12ef557a-81c0-4dc5-9d19-b76f07f6779d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "679\n",
            "['#PovertyPimps', '#YYJ boys \"and\" girls club south vanouver island', '1upparents anawim house', '1upparents avi', '1upparents bc housing', '1upparents boys \"and\" girls club south vanouver island', '900-block pandora avenue', 'ACEH', 'Aboriginal Coalition to End Homelessness', 'Addict', 'Addicted', 'Affordable', 'Affordable Housing', 'Affordable housing ', 'Alcoholic ', 'Anawim', 'BC Housing', 'Camp', 'Camper', 'Camping', 'Collude', 'Community Plan to End Homelessness ', 'Complex care ', 'Cool Aid', 'Coordinated Access and Assessment ', 'Couch-surfing', 'Crime ', 'Drugs', 'Emergency Housing ', 'Encampment', 'Encampments', 'Evict', 'Evicted', 'Eviction', 'Foundry Victoria ', 'Frontline', 'Greater Victoria Coalition to End Homelessness ', 'Greater Victoria Coalition to End Homelessness bc housing', 'Greater Victoria aceh', 'Greater Victoria aids vancouver island', 'Greater Victoria aryze developments', 'Greater Victoria avi', 'Greater Victoria bc housing', 'Greater Victoria beacon community services', 'Greater Victoria boys \"and\" girls club south vanouver island', 'Harm Reduction', 'Healthcare ', 'Home', 'Homeless', 'Homelessness ', 'Homelessness Services Association of BC', 'Housing', 'Lived Experience', 'Low income', 'Low-income', 'Makola Housing Society ', 'Narcotics ', 'Our Place', 'Overdose/overdosed ', 'Pacifica Housing ', 'Peer Housing Support', 'Peer Housing Support Program', 'Peers Victoria Resources Society ', 'People with lived experience', 'Person experiencing homelessness ', 'PiT', 'PiT Count', 'Point in Time', 'Poor ', 'Poverty', 'PovertyPimps', 'Reaching Home ', 'SOLID Outreach ', 'SVDP', 'Safe Supply ', 'Salvation Army', 'Service provider ', 'Shelter', 'Social Housing ', 'Social problem ', 'Social structure ', 'Society of St Vincent De Paul', 'Society of St Vincent De Paul Van Island', 'Sooke Homelessness Coalition', 'Sooke Transition House Society', 'Stolen', 'Subsidized Housing ', 'Substance use', 'Temporary housing ', 'Temporary shelter', 'Tent', 'The Cridge Centre for the Family ', 'Theft', 'Thief', 'Tiny Town', 'Umbrella Society ', 'Unhoused ', 'VWTH', 'Victoria B.C. aids vancouver island', 'Victoria B.C. anawim house', 'Victoria B.C. aryze developments', 'Victoria B.C. avi', 'Victoria B.C. bc housing', 'Victoria B.C. boys \"and\" girls club south vanouver island', 'Victoria Native Friendship Centre ', 'Victoria Sexual Assault Centre', 'Victoria Women in Need', \"Victoria Women's Transition House\", 'Victoria aceh', 'Victoria aids vancouver island', 'Victoria anawim house', 'Victoria aryze developments', 'Victoria avi', 'Victoria bc housing', 'Victoria beacon community services', 'Victoria boys \"and\" girls club south vanouver island', 'Victoria burnside gorge neighbourhood association', 'Victoria the aboriginal coalition to end homelessness', 'VictoriaBC aceh', 'VictoriaBC aids vancouver island', 'VictoriaBC anawim house', 'VictoriaBC avi', 'VictoriaBC bc housing', 'VictoriaBC boys \"and\" girls club south vanouver island', 'VictoriaBC burnside gorge neighbourhood association', 'VictoriaBC the aboriginal coalition to end homelessness', 'Violence', 'WiN', 'YYJ aceh', 'YYJ aids vancouver island', 'YYJ anawim house', 'YYJ aryze developments', 'YYJ avi', 'YYJ bc housing', 'YYJ beacon community services', 'YYJ boys \"and\" girls club south vanouver island', 'YYJ burnside gorge neighbourhood association', 'YYJ the aboriginal coalition to end homelessness', 'acehsociety', 'adam_stirling aceh', 'adam_stirling aids vancouver island', 'adam_stirling anawim house', 'adam_stirling aryze developments', 'adam_stirling avi', 'adam_stirling bc housing', 'adam_stirling beacon community services', 'adam_stirling boys \"and\" girls club south vanouver island', 'adam_stirling burnside gorge neighbourhood association', 'adampolsen', 'addict', 'addicted', 'adriandix', 'affordable', 'affordable housing', 'alanrycroft anawim house', 'alanrycroft aryze developments', 'alanrycroft avi', 'alanrycroft bc housing', 'alanrycroft boys \"and\" girls club south vanouver island', 'alanrycroft burnside gorge neighbourhood association', 'alcoholic', 'anawimhouse aids vancouver island', 'anawimhouse bc housing', 'anawimhouse boys \"and\" girls club south vanouver island', 'avivanisle', 'barbdesjardins', 'barclaywallace', 'bc_housing', 'beacon hill park', 'beaconhillfolks', 'beaconsave', 'berniepauly', 'brucealready avi', 'buckleygkml', 'burnside-gorge', 'burnsidegorge aceh', 'burnsidegorge aids vancouver island', 'burnsidegorge aryze developments', 'burnsidegorge avi', 'burnsidegorge beacon community services', 'burnsidegorge burnside gorge neighbourhood association', 'camp', 'camper', 'camping', 'carolinaibarra', 'cbcnews aceh', 'cbcnews anawim house', 'cbcnews aryze developments', 'cbcnews avi', 'cbcnews bc housing', 'cbcnews beacon community services', 'cbcnews boys \"and\" girls club south vanouver island', 'cbcnews burnside gorge neighbourhood association', 'cbcnews the aboriginal coalition to end homelessness', 'cecelia ravine park', 'central park', 'cfax1070', 'charlesbodi', 'chartj88', 'chek_news', 'chiefmanak', 'city of colwood', 'city of langford', 'city of victoria', 'cityofvictoria', 'coast salish', 'colinplant2018', 'collude', 'collusion', 'colwood', 'community plan to end homelessness', 'complex care', 'cook street village', 'cool aid society', 'coordinated access \"and\" assessment', 'coreyranger', 'coryresilient', 'couch surfing', 'councllrntaylor', 'crd_bc', 'crime', 'cspc_victoria avi', 'cspc_victoria bc housing', 'cspc_victoria boys \"and\" girls club south vanouver island', 'cspoliceservice', 'ctess1', 'ctvnewsvi', 'darylbassett3', 'dave_eby', 'davidhscreech', 'district of north saanich', 'district of saanich', 'downtown victoria business association', 'drcvictoria', 'drug use', 'drugs', 'dvba', 'elizabethmay', 'emergency housing', 'encampment', 'eric_doherty', 'esquimalt', 'esquimaltbc', 'esquimaltnation', 'evict', 'evicted', 'eviction', 'existenceprojct', 'extremeoutreach', 'fairfield-gonzales', 'fairfield_comm', 'fernwood', 'fernwood community association', 'fernwoodfca', 'firstmetvic', 'foul bay', 'foundry victoria', 'foundryvictoria', 'front line', 'front line worker', 'frontline', 'frontline worker', 'frontline worker ', 'galloping goose', 'goldstreamnews', 'gonzales', 'gonzales park', 'gracealore aceh', 'gracealore aids vancouver island', 'gracealore anawim house', 'gracealore aryze developments', 'gracealore avi', 'gracealore bc housing', 'gracealore beacon community services', 'gracealore boys \"and\" girls club south vanouver island', 'gracealore burnside gorge neighbourhood association', 'gracealore the aboriginal coalition to end homelessness', 'greater victoria acting together', 'greater victoria coalition to end homelessness', 'greater victoria housing society', 'habitat for humanity victoria', 'habitatvictoria', 'harm reduction', 'harris green', 'hattiecosta1', 'healthcare', 'hilarylmarks', 'hillside-quadra', 'hollywood park', 'homeforhope', 'homeless', 'homelessness', 'homelessness services association of bc', 'houseanawim', 'housesooke', 'housing', 'ilfp_victoria beacon community services', 'irving park', 'island health', 'islandcommha', 'jamesbaycp', 'jeremyloveday', 'jfatkey avi', 'jfatkey bc housing', 'johnhoward_can', 'juan de fuca', 'katstinson', 'kevinalbersbc aceh', 'kevinalbersbc aids vancouver island', 'kevinalbersbc avi', 'kevinalbersbc bc housing', 'kevinalbersbc boys \"and\" girls club south vanouver island', 'kristaloughton', 'kwakwaka’wakw', 'langford', 'laurel_bc', 'lekwungen', 'literacy victoria', 'lived experience', 'lochside', 'louise_hartland aceh', 'louise_hartland anawim house', 'louise_hartland aryze developments', 'louise_hartland avi', 'louise_hartland bc housing', 'louise_hartland burnside gorge neighbourhood association', 'low income', 'low-income', 'majatait', 'makola housing society', 'makolahousing', 'malahat aceh', 'malahat aids vancouver island', 'malahat anawim house', 'malahat aryze developments', 'malahat avi', 'malahat bc housing', 'malahat beacon community services', 'malahat boys \"and\" girls club south vanouver island', 'malahat burnside gorge neighbourhood association', 'malahat the aboriginal coalition to end homelessness', 'marikaalbert', 'mental health society of greater victoria', 'metchosin', 'mhrpsouthisland', 'mhsvictoria anawim house', 'mhsvictoria avi', 'mhsvictoria boys \"and\" girls club south vanouver island', 'millstream', 'murdochoakbay aceh', 'murdochoakbay anawim house', 'murdochoakbay aryze developments', 'murdochoakbay avi', 'murdochoakbay bc housing', 'murdochoakbay boys \"and\" girls club south vanouver island', 'murdochoakbay burnside gorge neighbourhood association', 'murdochoakbay the aboriginal coalition to end homelessness', 'murray_langdon', 'mustardseedvic anawim house', 'mustardseedvic avi', 'mustardseedvic bc housing', 'mustardseedvic burnside gorge neighbourhood association', 'mydvba', 'narcotics', 'need_2', 'nicolechaland', 'ninethreeseven', 'north park', 'nuu-chah-nulth', 'nuu-chah-nulth aceh', 'nuu-chah-nulth aids vancouver island', 'nuu-chah-nulth anawim house', 'nuu-chah-nulth aryze developments', 'nuu-chah-nulth avi', 'nuu-chah-nulth bc housing', 'nuu-chah-nulth beacon community services', 'nuu-chah-nulth boys \"and\" girls club south vanouver island', 'nuu-chah-nulth burnside gorge neighbourhood association', 'nuu-chah-nulth the aboriginal coalition to end homelessness', 'oak bay', 'oakbaynews', 'oaklands', 'oaklands park', 'ops', 'our place society', 'ourplacesociety', 'outreachsolid', 'overdose', 'overdosed', 'pacheedaht aceh', 'pacheedaht aids vancouver island', 'pacheedaht anawim house', 'pacheedaht aryze developments', 'pacheedaht avi', 'pacheedaht bc housing', 'pacheedaht beacon community services', 'pacheedaht boys \"and\" girls club south vanouver island', 'pacheedaht burnside gorge neighbourhood association', 'pacheedaht the aboriginal coalition to end homelessness', 'pacifica housing', 'pacificahousing', 'pacochran', 'pandora avenue', 'pauquachin aceh', 'pauquachin aids vancouver island', 'pauquachin anawim house', 'pauquachin aryze developments', 'pauquachin avi', 'pauquachin bc housing', 'pauquachin beacon community services', 'pauquachin boys \"and\" girls club south vanouver island', 'pauquachin burnside gorge neighbourhood association', 'peer housing support', 'peer housing support program', 'peers victoria resources society', 'peersvictoria', 'people with lived experience', 'person experiencing homelessness', 'photowarrior aceh', 'photowarrior aids vancouver island', 'photowarrior anawim house', 'photowarrior avi', 'photowarrior bc housing', 'photowarrior beacon community services', 'photowarrior boys \"and\" girls club south vanouver island', 'photowarrior the aboriginal coalition to end homelessness', 'pit', 'pit count', 'point ellice park', 'point in time', 'poor', 'poverty', 'povertypimps', 'quadra', 'quadra village', 'quadra village community centre', 'quadravillage', 'r_garrison', 'reaching home', 'red cedar café', 'rentsmart', 'rentsmartedu', 'restorative justice victoria', 'rock bay', 'rockheights', 'rockland', 'rotary club of victoria', 'royal athletic park', 'saanich', 'saanichnews', 'saanichpolice', 'saanichton', 'safe supply', 'safer victoria', 'safervic', 'safervic aceh', 'safervic bc housing', 'salarmyvicarc', 'salt spring island', 'saltspringx', 'salvation army victoria', 'sarahpottsvic aceh', 'sarahpottsvic aids vancouver island', 'sarahpottsvic anawim house', 'sarahpottsvic aryze developments', 'sarahpottsvic avi', 'sarahpottsvic bc housing', 'sarahpottsvic beacon community services', 'sarahpottsvic boys \"and\" girls club south vanouver island', 'sarahpottsvic burnside gorge neighbourhood association', 'sarahpottsvic the aboriginal coalition to end homelessness', \"sc'ianew aceh\", \"sc'ianew aids vancouver island\", \"sc'ianew anawim house\", \"sc'ianew aryze developments\", \"sc'ianew avi\", \"sc'ianew bc housing\", \"sc'ianew beacon community services\", 'sc\\'ianew boys \"and\" girls club south vanouver island', \"sc'ianew burnside gorge neighbourhood association\", \"sc'ianew the aboriginal coalition to end homelessness\", 'self_govern4us', 'selkirk green', 'selkirk trestle', 'service provider', 'shelliegudgeon', 'shelter', 'sidney', 'sjavicbc', 'soap4hopeyyj', 'social housing', 'social problem', 'social structure', 'solid outreach', 'songhees', 'songhees walkway', 'songheeschief', 'sooke', 'sooke homelessness coalition', 'sookecoalition', 'sookenews', 'south_island_c', 'spaynebc', 'spd_community', 'stadacona park', 'stephen_andrew', 'stolen', 'subsidized housing', 'substance use', 'substance uvic', 'substanceuvic', 'svdpvi', \"t'sou-ke aceh\", \"t'sou-ke aids vancouver island\", \"t'sou-ke anawim house\", \"t'sou-ke aryze developments\", \"t'sou-ke avi\", \"t'sou-ke bc housing\", \"t'sou-ke beacon community services\", 't\\'sou-ke boys \"and\" girls club south vanouver island', \"t'sou-ke burnside gorge neighbourhood association\", 'tagvictoriabc aceh', 'tagvictoriabc aids vancouver island', 'tagvictoriabc avi', 'tagvictoriabc bc housing', 'tagvictoriabc beacon community services', 'tagvictoriabc boys \"and\" girls club south vanouver island', 'tagvictoriabc burnside gorge neighbourhood association', 'tagvictoriabc the aboriginal coalition to end homelessness', 'talktoaryze', 'temporary shelter', 'tenant action group victoria', 'tent', 'the backpack project', 'the cridge centre for the family', 'the downtown victoria business association', 'the existence project', 'the gorge', 'the homeless ideas podcast', 'the john howard society of victoria', 'the mustard seed', 'the victoria foundation', 'the victoria real estate board', 'thebackpackpro1', 'thecridgecentre', 'theft', 'thief', 'timescolonist', 'togethervic', 'topaz park', 'tourismvi', 'town of view royal', 'township of esquimalt', 'tsartlip aceh', 'tsartlip aids vancouver island', 'tsartlip anawim house', 'tsartlip aryze developments', 'tsartlip avi', 'tsartlip bc housing', 'tsartlip beacon community services', 'tsartlip boys \"and\" girls club south vanouver island', 'tsartlip burnside gorge neighbourhood association', 'tsawout aceh', 'tsawout aids vancouver island', 'tsawout anawim house', 'tsawout aryze developments', 'tsawout avi', 'tsawout bc housing', 'tsawout beacon community services', 'tsawout boys \"and\" girls club south vanouver island', 'tsawout burnside gorge neighbourhood association', 'tsawout the aboriginal coalition to end homelessness', 'tseycum aceh', 'tseycum aids vancouver island', 'tseycum anawim house', 'tseycum aryze developments', 'tseycum avi', 'tseycum bc housing', 'tseycum beacon community services', 'tseycum boys \"and\" girls club south vanouver island', 'tseycum burnside gorge neighbourhood association', 'umbrella society', 'umbrellasociety', 'unhoused', 'united way southern vancouver island', 'unitedatoakbay', 'uplands', 'uvic aids vancouver island', 'uvic anawim house', 'uvic aryze developments', 'uvic avi', 'uvic beacon community services', 'vancouver island mental health society', 'vandupeople', 'vanislandhealth', 'varcsvictoria', 'vfamcourt aceh', 'vfamcourt avi', 'vfamcourt burnside gorge neighbourhood association', 'vibrant victoria', 'vic west', 'vic west park', 'viccoolaid', 'vicfoundation', 'vicpdcanada', 'vicplacemaking aceh', 'vicplacemaking aids vancouver island', 'vicplacemaking anawim house', 'vicplacemaking aryze developments', 'vicplacemaking avi', 'vicplacemaking bc housing', 'vicplacemaking beacon community services', 'vicplacemaking burnside gorge neighbourhood association', 'vicplacemaking the aboriginal coalition to end homelessness', 'victoria', 'victoria brain injury society', 'victoria chamber', 'victoria family court', 'victoria harbour cats', 'victoria native friendship centre', 'victoria placemaking', 'victoria ready', 'victoria real estate board', 'victoria sexual assault centre', 'victoria tenant action group', 'victoria west', 'victoria women in need', 'victoriabuzzes', 'victoriadra aceh', 'victoriadra bc housing', 'victoriadra the aboriginal coalition to end homelessness', 'victorianews aceh', 'victorianews aids vancouver island', 'victorianews anawim house', 'victorianews aryze developments', 'victorianews avi', 'victorianews bc housing', 'victorianews beacon community services', 'victorianews boys \"and\" girls club south vanouver island', 'victorianews burnside gorge neighbourhood association', 'victorianews the aboriginal coalition to end homelessness', 'victoriasandy', 'victoriavisitor aceh', 'victoriavisitor aids vancouver island', 'victoriavisitor burnside gorge neighbourhood association', 'victoriavisitor the aboriginal coalition to end homelessness', 'victoriaworkbc', 'victoriawth anawim house', 'vicyouthcouncil', 'view royal', 'violence', 'vreb', 'vreb aceh', 'vreb aids vancouver island', 'vreb anawim house', 'vreb avi', 'vreb bc housing', 'vreb beacon community services', 'vreb boys \"and\" girls club south vanouver island', 'wearenorthpark', 'west shore', 'workbc', 'workingupstream', 'wschamber1', 'wsáneć aceh', 'wsáneć aids vancouver island', 'wsáneć anawim house', 'wsáneć aryze developments', 'wsáneć avi', 'wsáneć bc housing', 'wsáneć beacon community services', 'wsáneć boys \"and\" girls club south vanouver island', 'wsáneć burnside gorge neighbourhood association', 'wsáneć the aboriginal coalition to end homelessness', 'youngparentssup', 'yyj_housing', 'yyjpolitics', 'zacdevries avi', 'zacdevries burnside gorge neighbourhood association']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import asyncpraw.models\n",
        "\n",
        "# async def fetch_data(reddit, subreddit_names, search_terms, limit_num=5000):\n",
        "#     titles = []\n",
        "#     texts = []\n",
        "#     user_ids = []\n",
        "#     comments = []\n",
        "#     subreddits = []\n",
        "#     used_search_terms = []\n",
        "\n",
        "#     # Ensure that subreddit_names is a list\n",
        "#     if not isinstance(subreddit_names, list):\n",
        "#         print(\"Error: subreddit_names should be a list.\")\n",
        "#         return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "#     # Ensure that search_terms is a list\n",
        "#     if not isinstance(search_terms, list):\n",
        "#         print(\"Error: search_terms should be a list.\")\n",
        "#         return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "#     for subreddit_name in subreddit_names:\n",
        "#         print(f\"\\nFetching data from subreddit: {subreddit_name}\")\n",
        "#         subreddit = await reddit.subreddit(subreddit_name)\n",
        "#         for search_term in search_terms:\n",
        "#             # Ensure each search_term is a string\n",
        "#             if not isinstance(search_term, str):\n",
        "#                 print(f\"Error: search_term '{search_term}' is not a string.\")\n",
        "#                 continue  # Skip this search term\n",
        "\n",
        "#             print(f\"Searching for term: {search_term}\")\n",
        "#             submissions = []\n",
        "#             async for submission in subreddit.search(search_term, limit=limit_num):\n",
        "#                 submissions.append(submission)\n",
        "#             print(f\"Found {len(submissions)} submissions for term '{search_term}' in subreddit '{subreddit_name}'\")\n",
        "\n",
        "#             for submission in submissions:\n",
        "#                 print(f\"Processing submission: {submission.id}\")\n",
        "#                 await asyncio.sleep(1)\n",
        "#                 await submission.load()\n",
        "#                 comment_queue = submission.comments[:]\n",
        "\n",
        "#                 comment_count = 0\n",
        "#                 while comment_queue:\n",
        "#                     comment = comment_queue.pop(0)\n",
        "#                     if isinstance(comment, asyncpraw.models.Comment):\n",
        "#                         titles.append(submission.title)\n",
        "#                         texts.append(submission.selftext)\n",
        "#                         user_ids.append(submission.author.name if submission.author else None)\n",
        "#                         comments.append(comment.body)\n",
        "#                         subreddits.append(subreddit_name)\n",
        "#                         used_search_terms.append(search_term)\n",
        "#                         comment_count += 1\n",
        "#                     elif isinstance(comment, asyncpraw.models.MoreComments):\n",
        "#                         more_comments = await comment.comments()\n",
        "#                         comment_queue.extend(more_comments)\n",
        "\n",
        "#                 print(f\"Appended {comment_count} comments for submission {submission.id}\")\n",
        "\n",
        "#     df = pd.DataFrame({\n",
        "#         'Subreddit': subreddits,\n",
        "#         'Title': titles,\n",
        "#         'Text': texts,\n",
        "#         'User ID': user_ids,\n",
        "#         'Comment': comments,\n",
        "#         'Search Term': used_search_terms\n",
        "#     })\n",
        "\n",
        "#     return df"
      ],
      "metadata": {
        "id": "LCJvscRL070m"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetch data function"
      ],
      "metadata": {
        "id": "J_q3yhfJSYGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# async def fetch_data(reddit, subreddit_names, search_terms, limit_num=10000):\n",
        "\n",
        "#     # API call counter and limit per time frame\n",
        "#     api_call_count = 0\n",
        "#     api_call_limit = 500  # Adjust based on Reddit's rate limit\n",
        "#     time_frame = 180\n",
        "\n",
        "#     for subreddit_name in subreddit_names:\n",
        "\n",
        "#         titles, texts, submission_ids, user_ids, used_search_terms = [], [], [], [], []\n",
        "#         seen_submission_ids = set()\n",
        "\n",
        "#         print(f\"\\nFetching data from subreddit: {subreddit_name}\")\n",
        "#         subreddit = await reddit.subreddit(subreddit_name)\n",
        "\n",
        "#         for search_term in search_terms:\n",
        "#             if not isinstance(search_term, str):\n",
        "#                 print(f\"Error: search_term '{search_term}' is not a string.\")\n",
        "#                 continue\n",
        "\n",
        "#             print(f\"Searching for term: {search_term}\")\n",
        "#             submissions = []\n",
        "\n",
        "#             # Increment API call count for the search query\n",
        "#             api_call_count += 1\n",
        "#             print(f\"\\nCurrent API call count: {api_call_count}\")\n",
        "#             if api_call_count >= api_call_limit:\n",
        "#                 print(f\"API call limit reached. Waiting for {time_frame} seconds.\")\n",
        "#                 await asyncio.sleep(time_frame)\n",
        "#                 api_call_count = 0  # Reset counter after waiting\n",
        "\n",
        "#             async for submission in subreddit.search(search_term, limit=limit_num):\n",
        "#                 if submission.id in seen_submission_ids:\n",
        "#                   continue\n",
        "\n",
        "#                 submissions.append(submission)\n",
        "#                 api_call_count += 1\n",
        "#                 print(f\"\\nCurrent API call count: {api_call_count}\")\n",
        "\n",
        "#                 if api_call_count >= api_call_limit:\n",
        "#                     print(f\"API call limit reached. Waiting for {time_frame} seconds.\")\n",
        "#                     await asyncio.sleep(time_frame)\n",
        "#                     api_call_count = 0  # Reset counter after waiting\n",
        "\n",
        "#             print(f\"Found {len(submissions)} submissions for term '{search_term}' in subreddit '{subreddit_name}'\")\n",
        "\n",
        "#             for submission in submissions:\n",
        "#                 print(f\"Processing submission: {submission.id}\")\n",
        "#                 await submission.load()\n",
        "\n",
        "#                 titles.append(submission.title)\n",
        "#                 texts.append(submission.selftext)\n",
        "#                 submission_ids.append(submission.id)\n",
        "#                 user_ids.append(submission.author.name if submission.author else None)\n",
        "#                 used_search_terms.append(search_term)\n",
        "#                 seen_submission_ids.add(submission.id)\n",
        "\n",
        "#                 api_call_count += 1\n",
        "#                 print(f\"\\nCurrent API call count: {api_call_count}\")\n",
        "#                 if api_call_count >= api_call_limit:\n",
        "#                     print(f\"API call limit reached. Waiting for {time_frame} seconds.\")\n",
        "#                     await asyncio.sleep(time_frame)\n",
        "#                     api_call_count = 0  # Reset counter after waiting\n",
        "\n",
        "#         # Create a DataFrame for the current subreddit\n",
        "#         subreddit_df = pd.DataFrame({\n",
        "#             'Subreddit': [subreddit_name] * len(titles),\n",
        "#             'Title': titles,\n",
        "#             'Text': texts,\n",
        "#             'Submission ID': submission_ids,\n",
        "#             'User ID': user_ids,\n",
        "#             'Search Term': used_search_terms\n",
        "#         })\n",
        "\n",
        "#         subreddit_df = subreddit_df.drop_duplicates()\n",
        "\n",
        "#         # Save to CSV in Google Drive\n",
        "#         file_path = f'/content/drive/My Drive/SWB-GVCEH/{subreddit_name}_data.csv'\n",
        "#         subreddit_df.to_csv(file_path, index=False)\n",
        "#         print(f\"Data for {subreddit_name} saved to {file_path}\")\n",
        "\n",
        "#     return pd.DataFrame()  # Or any other relevant return statement\n",
        "\n",
        "# # Remember to call this function with await\n",
        "# # df = await fetch_data(reddit, subreddit_names, search_terms, limit_num)"
      ],
      "metadata": {
        "id": "05uAEuxDI7PR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def fetch_data(reddit, subreddit_names, search_terms, limit_num=10000):\n",
        "    api_call_times = deque(maxlen=1000)  # Store timestamps of the last 1000 API calls\n",
        "\n",
        "    for subreddit_name in subreddit_names:\n",
        "\n",
        "        subreddit = await reddit.subreddit(subreddit_name)\n",
        "\n",
        "        file_path = f'/content/drive/My Drive/SWB-GVCEH/{subreddit_name}_data.csv'\n",
        "        last_written_index = 0  # Initialize last written index\n",
        "\n",
        "        try:\n",
        "            subreddit_df = pd.read_csv(file_path)\n",
        "            seen_submission_ids = set(subreddit_df['Submission ID'])\n",
        "        except FileNotFoundError:\n",
        "            subreddit_df = pd.DataFrame(columns=['Subreddit', 'Title', 'Text', 'Submission ID', 'User ID', 'Search Term'])\n",
        "            seen_submission_ids = set()\n",
        "\n",
        "        for search_term in search_terms:\n",
        "            if not isinstance(search_term, str):\n",
        "                print(f\"Error: search_term '{search_term}' is not a string.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nFetching data for term: {search_term} in subreddit: {subreddit_name}\")\n",
        "\n",
        "            # Check rate limit before making a search\n",
        "            while api_call_times and len(api_call_times) == api_call_times.maxlen and (datetime.now() - api_call_times[0]) < timedelta(minutes=10):\n",
        "                wait_time = (timedelta(minutes=10) - (datetime.now() - api_call_times[0])).total_seconds()\n",
        "                print(f\"Rate limit approaching. Waiting for {wait_time:.2f} seconds.\")\n",
        "                await asyncio.sleep(wait_time)\n",
        "\n",
        "            async for submission in subreddit.search(search_term, limit=limit_num):\n",
        "                # Record the API call time\n",
        "                api_call_times.append(datetime.now())\n",
        "\n",
        "                if submission.id in seen_submission_ids:\n",
        "                    continue\n",
        "\n",
        "                print(f\"Processing submission: {submission.id}, API calls in the last 10 min: {len(api_call_times)}\")\n",
        "                await submission.load()\n",
        "                seen_submission_ids.add(submission.id)\n",
        "\n",
        "                # Process and append new submission data\n",
        "                new_data = pd.DataFrame({\n",
        "                    'Subreddit': [subreddit_name],\n",
        "                    'Title': [submission.title],\n",
        "                    'Text': [submission.selftext],\n",
        "                    'Submission ID': [submission.id],\n",
        "                    'User ID': [submission.author.name if submission.author else None],\n",
        "                    'Search Term': [search_term]\n",
        "                })\n",
        "                subreddit_df = pd.concat([subreddit_df, new_data], ignore_index=True)\n",
        "\n",
        "                # Check if there are at least 500 new rows to append\n",
        "                if len(subreddit_df) >= last_written_index + 500:\n",
        "                    new_rows = subreddit_df.iloc[last_written_index:last_written_index + 500]\n",
        "                    new_rows.to_csv(file_path, mode='a', index=False, header=last_written_index == 0)\n",
        "                    last_written_index += len(new_rows)  # Update the last written index\n",
        "                    print(f\"Intermediate data appended for subreddit: {subreddit_name} after {len(subreddit_df)} rows.\")\n",
        "\n",
        "            # Final append for any remaining rows after the last intermittent save\n",
        "            if len(subreddit_df) > last_written_index:\n",
        "                new_rows = subreddit_df.iloc[last_written_index:]\n",
        "                new_rows.to_csv(file_path, mode='a', index=False, header=last_written_index == 0)\n",
        "                print(f\"Final data appended for subreddit: {subreddit_name}\")\n",
        "\n",
        "    print(\"Data fetching complete.\")\n",
        "    # return subreddit_df\n",
        "\n",
        "# Remember to call this function with await\n",
        "# df = await fetch_data(reddit, subreddit_names, search_terms, limit_num)"
      ],
      "metadata": {
        "id": "tFm67WlTB5pt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # df = await fetch_data(reddit, subreddits_list, search_terms)\n",
        "    await fetch_data(reddit, subreddits_list, search_terms)\n",
        "\n",
        "    await reddit.close()\n",
        "    # return df\n",
        "\n",
        "# Run the main function and get the DataFrame\n",
        "# reddit_data_search_terms = await main()\n",
        "await main()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "# Path to the CSV file\n",
        "# file_path = '/content/SWB-GVCEH/data/reddit_data_search_terms.csv'\n",
        "# file_path = '/content/drive/My Drive/SWB-GVCEH/data/reddit_data_search_terms.csv'\n",
        "\n",
        "# reddit_data_search_terms = reddit_data_search_terms.drop_duplicates().reset_index(drop=True)\n",
        "# reddit_data_search_terms.to_csv(file_path, index=False)\n",
        "# print(\"Data fetched and saved to reddit_data_search_terms.csv\")\n",
        "\n",
        "# # Trigger a download to your local machine\n",
        "# files.download(file_path)\n",
        "\n",
        "# # Set your git remote URL to include the PAT for authentication\n",
        "# repo_url = 'https://github.com/alex-jk/SWB-GVCEH.git'  # Replace with your repository's URL\n",
        "# pat = os.environ['GITHUB_PAT']\n",
        "# repo_url_with_token = repo_url[:8] + pat + \"@\" + repo_url[8:]\n",
        "\n",
        "# !git remote set-url origin {repo_url_with_token}\n",
        "\n",
        "# # Navigate to the repository directory, add, commit, and push the new CSV file\n",
        "# %cd /content/SWB-GVCEH\n",
        "# !git add 'data/reddit_data_search_terms.csv'\n",
        "# !git commit -m \"Add fetched Reddit data search terms CSV\"\n",
        "# !git push origin main  # Replace 'main' with your branch name if it's different\n",
        "\n",
        "# # Reset the remote URL to the original without the PAT\n",
        "# !git remote set-url origin {repo_url}\n",
        "# print(\"CSV file pushed to GitHub.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "79qRrHRP1CX8",
        "outputId": "fe2c89f3-5965-4aee-c3cb-3e091af6917b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching data for term: #PovertyPimps in subreddit: VictoriaBC\n",
            "Final data appended for subreddit: VictoriaBC\n",
            "\n",
            "Fetching data for term: #YYJ boys \"and\" girls club south vanouver island in subreddit: VictoriaBC\n",
            "Final data appended for subreddit: VictoriaBC\n",
            "\n",
            "Fetching data for term: 1upparents anawim house in subreddit: VictoriaBC\n",
            "Processing submission: 17yjdiw, API calls in the last 10 min: 3\n",
            "Processing submission: 7h0e58, API calls in the last 10 min: 4\n",
            "Processing submission: 169ywlj, API calls in the last 10 min: 5\n",
            "Processing submission: ii24i3, API calls in the last 10 min: 6\n",
            "Processing submission: 10hfalw, API calls in the last 10 min: 7\n",
            "Processing submission: z5003f, API calls in the last 10 min: 8\n",
            "Processing submission: 1jlwiq, API calls in the last 10 min: 9\n",
            "Final data appended for subreddit: VictoriaBC\n",
            "\n",
            "Fetching data for term: 1upparents avi in subreddit: VictoriaBC\n",
            "Final data appended for subreddit: VictoriaBC\n",
            "\n",
            "Fetching data for term: 1upparents bc housing in subreddit: VictoriaBC\n",
            "Processing submission: 16s87oq, API calls in the last 10 min: 10\n",
            "Processing submission: 10kihls, API calls in the last 10 min: 11\n",
            "Processing submission: 15cye8g, API calls in the last 10 min: 12\n",
            "Processing submission: k8klqj, API calls in the last 10 min: 13\n",
            "Processing submission: sui0lf, API calls in the last 10 min: 14\n",
            "Processing submission: zivxfd, API calls in the last 10 min: 15\n",
            "Processing submission: vod2af, API calls in the last 10 min: 16\n",
            "Processing submission: qgbioz, API calls in the last 10 min: 17\n",
            "Processing submission: mf2cur, API calls in the last 10 min: 18\n",
            "Processing submission: 14n4ys0, API calls in the last 10 min: 19\n",
            "Processing submission: rak94x, API calls in the last 10 min: 20\n",
            "Processing submission: 14yo7kb, API calls in the last 10 min: 21\n",
            "Processing submission: 16t1ck0, API calls in the last 10 min: 22\n",
            "Processing submission: 11z1wuk, API calls in the last 10 min: 23\n",
            "Processing submission: hr3fra, API calls in the last 10 min: 24\n",
            "Processing submission: ql6mhi, API calls in the last 10 min: 25\n",
            "Processing submission: fsr24x, API calls in the last 10 min: 26\n",
            "Processing submission: 16ovaso, API calls in the last 10 min: 27\n",
            "Processing submission: g83ega, API calls in the last 10 min: 28\n",
            "Processing submission: 6f7xp8, API calls in the last 10 min: 29\n",
            "Processing submission: umm0cf, API calls in the last 10 min: 30\n",
            "Processing submission: 157n3wv, API calls in the last 10 min: 31\n",
            "Processing submission: 144ljba, API calls in the last 10 min: 32\n",
            "Processing submission: 1201wfz, API calls in the last 10 min: 33\n",
            "Processing submission: 17gzax0, API calls in the last 10 min: 34\n",
            "Processing submission: 1830wwy, API calls in the last 10 min: 35\n",
            "Processing submission: 17rqkrb, API calls in the last 10 min: 36\n",
            "Processing submission: 10lxb44, API calls in the last 10 min: 37\n",
            "Processing submission: vhntoy, API calls in the last 10 min: 38\n",
            "Processing submission: iq2i8w, API calls in the last 10 min: 39\n",
            "Processing submission: uijt2w, API calls in the last 10 min: 40\n",
            "Processing submission: 90n5tp, API calls in the last 10 min: 41\n",
            "Processing submission: pknoz5, API calls in the last 10 min: 42\n",
            "Processing submission: 8bafjr, API calls in the last 10 min: 43\n",
            "Processing submission: 16olp2h, API calls in the last 10 min: 44\n",
            "Processing submission: 2k7toj, API calls in the last 10 min: 45\n",
            "Processing submission: 11f2vq, API calls in the last 10 min: 46\n",
            "Processing submission: 10m4cz7, API calls in the last 10 min: 47\n",
            "Processing submission: 17n01sm, API calls in the last 10 min: 48\n",
            "Processing submission: kvdr9o, API calls in the last 10 min: 49\n",
            "Processing submission: s3yj6m, API calls in the last 10 min: 50\n",
            "Processing submission: 4l7qmm, API calls in the last 10 min: 51\n",
            "Processing submission: q971ks, API calls in the last 10 min: 52\n",
            "Processing submission: wg97tt, API calls in the last 10 min: 53\n",
            "Processing submission: 13zwdo5, API calls in the last 10 min: 54\n",
            "Processing submission: 1622w22, API calls in the last 10 min: 55\n",
            "Processing submission: 11sbbvb, API calls in the last 10 min: 56\n",
            "Processing submission: 167ke58, API calls in the last 10 min: 57\n",
            "Processing submission: pwqu5d, API calls in the last 10 min: 58\n",
            "Processing submission: tgqqi4, API calls in the last 10 min: 59\n",
            "Processing submission: wpglei, API calls in the last 10 min: 60\n",
            "Processing submission: 2cvqce, API calls in the last 10 min: 61\n",
            "Processing submission: 10kqynl, API calls in the last 10 min: 62\n",
            "Processing submission: 11ytlkc, API calls in the last 10 min: 63\n",
            "Processing submission: lwab4d, API calls in the last 10 min: 64\n",
            "Processing submission: 6alhgc, API calls in the last 10 min: 65\n",
            "Processing submission: iob63a, API calls in the last 10 min: 66\n",
            "Processing submission: 6z5sm5, API calls in the last 10 min: 67\n",
            "Processing submission: ze7xhm, API calls in the last 10 min: 68\n",
            "Processing submission: 10l6xmx, API calls in the last 10 min: 69\n",
            "Processing submission: 14rj83v, API calls in the last 10 min: 70\n",
            "Processing submission: vi8rah, API calls in the last 10 min: 71\n",
            "Processing submission: x08bnk, API calls in the last 10 min: 72\n",
            "Processing submission: z0b3ug, API calls in the last 10 min: 73\n",
            "Processing submission: 10dtwgm, API calls in the last 10 min: 74\n",
            "Processing submission: 9pywbb, API calls in the last 10 min: 75\n",
            "Processing submission: 11szhf0, API calls in the last 10 min: 76\n",
            "Processing submission: ue5d4w, API calls in the last 10 min: 77\n",
            "Processing submission: qu6odz, API calls in the last 10 min: 78\n",
            "Processing submission: ph9v0w, API calls in the last 10 min: 79\n",
            "Processing submission: oyv0v4, API calls in the last 10 min: 80\n",
            "Processing submission: yp27qr, API calls in the last 10 min: 81\n",
            "Processing submission: vsyx8p, API calls in the last 10 min: 82\n",
            "Processing submission: x0ryyq, API calls in the last 10 min: 83\n",
            "Processing submission: 9nc3w8, API calls in the last 10 min: 84\n",
            "Processing submission: xu7v8i, API calls in the last 10 min: 85\n",
            "Processing submission: rudhas, API calls in the last 10 min: 86\n",
            "Processing submission: agcmq0, API calls in the last 10 min: 87\n",
            "Processing submission: 107kwpw, API calls in the last 10 min: 88\n",
            "Processing submission: 11rncon, API calls in the last 10 min: 89\n",
            "Processing submission: xxi7u9, API calls in the last 10 min: 90\n",
            "Processing submission: 8zwd2d, API calls in the last 10 min: 91\n",
            "Processing submission: 9ftlnj, API calls in the last 10 min: 92\n",
            "Processing submission: 9eg7kl, API calls in the last 10 min: 93\n",
            "Processing submission: kn75qc, API calls in the last 10 min: 94\n",
            "Processing submission: 13hh3rf, API calls in the last 10 min: 95\n",
            "Processing submission: zs4wq2, API calls in the last 10 min: 96\n",
            "Processing submission: 13pexd7, API calls in the last 10 min: 97\n",
            "Processing submission: sc6dpk, API calls in the last 10 min: 98\n",
            "Processing submission: tb7kqc, API calls in the last 10 min: 99\n",
            "Processing submission: eehktp, API calls in the last 10 min: 100\n",
            "Processing submission: 1235hvz, API calls in the last 10 min: 101\n",
            "Processing submission: y8of1u, API calls in the last 10 min: 102\n",
            "Processing submission: 10m1b43, API calls in the last 10 min: 103\n",
            "Processing submission: zo2mua, API calls in the last 10 min: 104\n",
            "Processing submission: s7681u, API calls in the last 10 min: 105\n",
            "Processing submission: krjqji, API calls in the last 10 min: 106\n",
            "Processing submission: q84uoj, API calls in the last 10 min: 107\n",
            "Processing submission: sunalg, API calls in the last 10 min: 108\n",
            "Processing submission: a1uxmu, API calls in the last 10 min: 109\n",
            "Processing submission: j37gl0, API calls in the last 10 min: 110\n",
            "Processing submission: iffut6, API calls in the last 10 min: 111\n",
            "Processing submission: rwvueb, API calls in the last 10 min: 112\n",
            "Processing submission: 15normb, API calls in the last 10 min: 113\n",
            "Processing submission: k0ios6, API calls in the last 10 min: 114\n",
            "Processing submission: fmwpmm, API calls in the last 10 min: 115\n",
            "Processing submission: u6h6d7, API calls in the last 10 min: 116\n",
            "Processing submission: lk7on6, API calls in the last 10 min: 117\n",
            "Processing submission: 6y1kzj, API calls in the last 10 min: 118\n",
            "Processing submission: y0utth, API calls in the last 10 min: 119\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-4db50ba23aca>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run the main function and get the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# reddit_data_search_terms = await main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save the DataFrame to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-4db50ba23aca>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# df = await fetch_data(reddit, subreddits_list, search_terms)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubreddits_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mawait\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-10f63b5950a6>\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(reddit, subreddit_names, search_terms, limit_num)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing submission: {submission.id}, API calls in the last 10 min: {len(api_call_times)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Process and append new submission data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/models/reddit/base.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/util/deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At most one of 'data' or 'json' is supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             return await self._core.request(\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         return await self._request_with_retries(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mretry_strategy_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         response, saved_exception = await self._make_request(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    185\u001b[0m     ):\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             response = await self._rate_limiter.call(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             return await self._http.request(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    584\u001b[0m                             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                                 \u001b[0;32mawait\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                                 \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m                     \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                     \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpProcessingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                     raise ClientResponseError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.listdir('/content/drive/My Drive/')\n",
        "# !apt-get install git-lfs"
      ],
      "metadata": {
        "id": "Bcnqm7sp-rkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1339472d-0cba-412a-e0de-34b32da277e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IMG_20160710_163910.jpg',\n",
              " '48CordellaAve - signed offer 1.gdoc',\n",
              " '20190324_143215.jpg',\n",
              " 'RUL slides Sasha added v2.gslides',\n",
              " 'MRP Proposal - Eugenia Silina.docx',\n",
              " 'Alexandra Joukova MRP Proposal.gdoc',\n",
              " 'MRP Proposal - Eugenia Silina.gdoc',\n",
              " 'Ontario Poisonous Mushrooms.gdoc',\n",
              " 'MRP 2020 Poster Template.gslides',\n",
              " 'Alexandra Joukova MRP 2021 Poster.gslides',\n",
              " 'Alexandra Joukova_Fire Incidents Prediction Model.gslides',\n",
              " 'Subreddit LDA.gsheet',\n",
              " 'costs.gsheet',\n",
              " 'NFT',\n",
              " 'Retainer.pdf',\n",
              " 'Retainer.gdoc',\n",
              " 'Cottage',\n",
              " 'Landlord letter.docx',\n",
              " '48 Cordella Property Tax 2021.gdoc',\n",
              " 'Personal Tax summary - checklist 2021.xlsx',\n",
              " '20220603_213302.jpg',\n",
              " 'Cordella IKEA Receipt Dec 2021.jpg',\n",
              " 'Cordella IKEA Receipt Mar 2021.jpg',\n",
              " 'Cordella IKEA Receipt Jul 2021.jpg',\n",
              " 'Alexandra Joukova T183.pdf',\n",
              " 'Alexandra Joukova T183.gdoc',\n",
              " 'DS materials and links.gsheet',\n",
              " 'Alexandra Joukova Resume 2022.gdoc',\n",
              " 'Resume_SashaJoukova_feedback.docx',\n",
              " 'DS Notes.gdoc',\n",
              " 'Leo Misyura Birth Certificate.gdoc',\n",
              " 'Contact Information and Informed Concent.pdf',\n",
              " 'eClaims-Patient-Consent-Form.pdf',\n",
              " 'Contact Information and Informed Concent.gdoc',\n",
              " 'Untitled document (1).gdoc',\n",
              " 'L3A Study_LOI & Consent_Oct 2022.gdoc',\n",
              " 'NYC bike project.ipynb',\n",
              " 'Colab Notebooks',\n",
              " 'NYC bike project - Updated.ipynb',\n",
              " 'Caregiver Contact Info.gsheet',\n",
              " '20230413_142754 (2).jpg',\n",
              " '20230413_141034 (1).jpg',\n",
              " '20230413_143352 (1).jpg',\n",
              " '20230413_143614 (1).jpg',\n",
              " '20230413_143809 (1).jpg',\n",
              " '20230413_143853.jpg',\n",
              " '20230413_143926.jpg',\n",
              " '20230413_144001.jpg',\n",
              " '20230413_144030 (1).jpg',\n",
              " '20230413_142754 (1).jpg',\n",
              " '20230413_141034.jpg',\n",
              " '20230413_143352.jpg',\n",
              " '20230413_143809.jpg',\n",
              " '20230413_143614.jpg',\n",
              " '20230413_142754.jpg',\n",
              " '20230413_144030.jpg',\n",
              " 'Basement Ad',\n",
              " 'Taxes 2022.gsheet',\n",
              " '36 Levitt Court Rooming House Ad.pdf',\n",
              " 'Cottage Trailer Parks List.gsheet',\n",
              " 'Untitled document.gdoc',\n",
              " '36 Levitt Court Rooming House Ad.gdoc',\n",
              " 'Trip items prep.gsheet',\n",
              " 'DS Preparation.gdoc',\n",
              " 'Alexandra Joukova CV 2023.gdoc',\n",
              " 'MRP Project',\n",
              " 'Alexandra Joukova Cover Letter.gdoc',\n",
              " 'Rentals Data',\n",
              " 'Untitled spreadsheet (3).gsheet',\n",
              " 'Hiking Spots Google Maps.gsheet',\n",
              " 'Alexandra Joukova CL and Resume DS.gdoc',\n",
              " 'Alexandra Joukova Resume.gdoc',\n",
              " 'Alexandra Joukova Resume Economics.gdoc',\n",
              " 'Alexandra Joukova Cover Letter Economics.gdoc',\n",
              " 'Ontario Air Pollution',\n",
              " 'Alexandra Joukova Resume V2.gdoc',\n",
              " 'Copy of Alexandra Joukova Resume V3.gdoc',\n",
              " 'Untitled spreadsheet (2).gsheet',\n",
              " 'Take Home Challenge II.docx',\n",
              " 'quarterly-income.gsheet',\n",
              " 'FDIC_balance_sheet.gsheet',\n",
              " 'Skills Matrix.gsheet',\n",
              " 'Macroecon_Vars_Data.gsheet',\n",
              " 'Macroecon_Vars_Daily.gsheet',\n",
              " 'balance-sheet.gsheet',\n",
              " 'valid_combinations (1).gsheet',\n",
              " 'valid_combinations.csv',\n",
              " 'BMO_merged_data.csv',\n",
              " 'BMO_merged_data.gsheet',\n",
              " 'valid_combinations.gsheet',\n",
              " 'BMO_model_summary_Tim.csv',\n",
              " 'BMO_model_summary_IB_.csv',\n",
              " 'BMO_model_summary_Non.csv',\n",
              " 'BMO_model_summary_Tim.gsheet',\n",
              " 'Untitled spreadsheet (1).gsheet',\n",
              " 'formatted_output_Tim.csv',\n",
              " 'formatted_output_Tim.gsheet',\n",
              " 'formatted_Tim (1).gsheet',\n",
              " 'formatted_Tim.csv',\n",
              " 'formatted_Tim.gsheet',\n",
              " 'formatted_IB_.csv',\n",
              " 'formatted_IB_.gsheet',\n",
              " 'formatted_Non.csv',\n",
              " 'formatted_Non.gsheet',\n",
              " 'Alexandra Joukova - BMO Take Home Challenge.gdoc',\n",
              " 'Leo_Misyura_Spee h_Therapy_20231017_173619.jpg',\n",
              " 'Fire Incidents Prediction',\n",
              " 'quantile_regression_plots.py',\n",
              " 'Quantile Regression.pptx',\n",
              " 'Candidate Reference Check Consent Form.pdf',\n",
              " 'Alexandra Joukova Cover Letter DS.gdoc',\n",
              " 'Alexandra Joukova Resume Anon.gdoc',\n",
              " 'Bank.Green',\n",
              " 'Competitor Research',\n",
              " 'Untitled spreadsheet.gsheet',\n",
              " 'Ontario PM2.5 Historical Values.gdoc',\n",
              " 'Alexandra Joukova Resume V4.gdoc',\n",
              " 'SWB-GVCEH',\n",
              " 'Competitor User Reviews Text Analysis.gsheet',\n",
              " 'Metrolinx Assignment']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git lfs track 'data/reddit_data_search_terms.csv'"
      ],
      "metadata": {
        "id": "5F6u7Uye-vFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git add .gitattributes 'data/reddit_data_search_terms.csv'\n",
        "# !git commit -m \"Add large CSV file with Git LFS\"\n",
        "\n",
        "# repo_url = 'https://github.com/alex-jk/SWB-GVCEH.git'  # Replace with your repository's URL\n",
        "# repo_url_with_token = repo_url[:8] + pat + \"@\" + repo_url[8:]\n",
        "# !git remote set-url origin {repo_url_with_token}\n",
        "\n",
        "# !git push origin main"
      ],
      "metadata": {
        "id": "-XVNG8Eu-yyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git filter-branch --force --index-filter \\\n",
        "#   \"git rm --cached --ignore-unmatch data/reddit_data_search_terms.csv\" \\\n",
        "#   --prune-empty --tag-name-filter cat -- --all\n",
        "# !git push origin main --force\n"
      ],
      "metadata": {
        "id": "DQk7GVtk_Kj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load reddit data file\n",
        "##### Since the file is too large to be pushed to the git repo, it was uploaded to Google drive"
      ],
      "metadata": {
        "id": "weyYVg4RDYGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/SWB-GVCEH/reddit_data_search_terms.csv'  # Update with the path to your CSV file in Google Drive\n",
        "reddit_data_search_df = pd.read_csv(file_path)\n",
        "print(reddit_data_search_df.shape)\n",
        "print(reddit_data_search_df.columns)\n",
        "print(reddit_data_search_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbIhMF-oDaMd",
        "outputId": "4a97cba6-6341-428f-95b5-a6fc9b22f3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(159128, 5)\n",
            "Index(['Subreddit', 'Title', 'Text', 'User ID', 'Comment'], dtype='object')\n",
            "   Subreddit                                              Title  \\\n",
            "0      Sooke                                   Camping in Sooke   \n",
            "1      Sooke                                   Camping in Sooke   \n",
            "2      Sooke                                   Camping in Sooke   \n",
            "3      Sooke                                   Camping in Sooke   \n",
            "4      Sooke  Sooke boy denied $19,000 per month drug for th...   \n",
            "5      Sooke        Are power outages in Sooke a regular thing?   \n",
            "6      Sooke        Are power outages in Sooke a regular thing?   \n",
            "7      Sooke        Are power outages in Sooke a regular thing?   \n",
            "8      Sooke        Are power outages in Sooke a regular thing?   \n",
            "9      Sooke        Are power outages in Sooke a regular thing?   \n",
            "10     Sooke  Help: What’s the best option for housing right...   \n",
            "11     Sooke  Help: What’s the best option for housing right...   \n",
            "12     Sooke  Help: What’s the best option for housing right...   \n",
            "13     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "14     Sooke  Cockatoo escaped across from Saseenos. Please ...   \n",
            "15     Sooke  Cockatoo escaped across from Saseenos. Please ...   \n",
            "16     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "17     Sooke  Sooke Tourism Association: “Time to show off y...   \n",
            "18     Sooke  Help: What’s the best option for housing right...   \n",
            "19     Sooke  Help: What’s the best option for housing right...   \n",
            "\n",
            "                                                 Text         User ID  \\\n",
            "0   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "1   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "2   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "3   Hi, we’re planning on camping in Sooke on Apri...    VanillaWrong   \n",
            "4                                                 NaN  TrueNorthGreen   \n",
            "5   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "6   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "7   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "8   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "9   Greetings, people of Sooke!\\nMy wife and I are...         Pkard82   \n",
            "10  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "11  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "12  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "13                                                NaN     AMadcapLass   \n",
            "14  3 days ago my friends cockatoo escaped from he...        babetteq   \n",
            "15  3 days ago my friends cockatoo escaped from he...        babetteq   \n",
            "16                                                NaN     AMadcapLass   \n",
            "17                                                NaN     AMadcapLass   \n",
            "18  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "19  Looking for at least a two bedroom, preferably...     redd_planet   \n",
            "\n",
            "                                              Comment  \n",
            "0   Check on the BC parks website to see if China ...  \n",
            "1   Just a heads up that there is a government man...  \n",
            "2   Check on the BC parks website to see if China ...  \n",
            "3   Just a heads up that there is a government man...  \n",
            "4   there are other treatments for this, i'm just ...  \n",
            "5   Lived here for a year. The power has probably ...  \n",
            "6   It depends where you’re at. In Sooke core, sun...  \n",
            "7   Yes, expect your power to go out several times...  \n",
            "8   Ah that's all good to know! Kinda what I'd exp...  \n",
            "9   Summer it’s rare, winter months it’s very common.  \n",
            "10  About the same options for those of us already...  \n",
            "11                                          [deleted]  \n",
            "12  Its better to buy than rent if you can afford ...  \n",
            "13  Would you post an address and contact phone nu...  \n",
            "14     I'll keep my eyes and ears open; best of luck!  \n",
            "15  Thank you.  Today was first of no sightings. W...  \n",
            "16  Would you post an address and contact phone nu...  \n",
            "17  Would you post an address and contact phone nu...  \n",
            "18  About the same options for those of us already...  \n",
            "19                                          [deleted]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to concatenate title, text, and comments\n",
        "def concatenate_post_data(group):\n",
        "    # Concatenate the title and text\n",
        "    title_text = group['Title'].iloc[0] + ' ' + group['Text'].iloc[0]\n",
        "\n",
        "    # Concatenate all comments\n",
        "    comments = ' '.join(group['Comment'].astype(str))\n",
        "\n",
        "    # Full document (title, text, and comments)\n",
        "    full_document = title_text + ' ' + comments\n",
        "\n",
        "    return pd.Series([title_text, full_document], index=['TitleText', 'Document'])\n",
        "\n",
        "# Group by the post identifiers and apply the concatenation function\n",
        "grouped = reddit_data_search_df.groupby(['Subreddit', 'Title', 'Text', 'User ID'])\n",
        "documents_df = grouped.apply(concatenate_post_data).reset_index()\n",
        "\n",
        "# Displaying the first few rows\n",
        "print(documents_df.shape)\n",
        "print(documents_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "qlOqzVB67Krt",
        "outputId": "7032fcbb-5b83-432f-ba6d-74617eaf5e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3818, 6)\n",
            "  Subreddit                                              Title  \\\n",
            "0     Sooke        Are power outages in Sooke a regular thing?   \n",
            "1     Sooke  Are there sexual predator watchdog groups in S...   \n",
            "2     Sooke                                   Camping in Sooke   \n",
            "3     Sooke  Cockatoo escaped across from Saseenos. Please ...   \n",
            "4     Sooke  Help: What’s the best option for housing right...   \n",
            "\n",
            "                                                Text       User ID  \\\n",
            "0  Greetings, people of Sooke!\\nMy wife and I are...       Pkard82   \n",
            "1  My girlfriend was recently traveling on Vancou...  scoobysmokes   \n",
            "2  Hi, we’re planning on camping in Sooke on Apri...  VanillaWrong   \n",
            "3  3 days ago my friends cockatoo escaped from he...      babetteq   \n",
            "4  Looking for at least a two bedroom, preferably...   redd_planet   \n",
            "\n",
            "                                           TitleText  \\\n",
            "0  Are power outages in Sooke a regular thing? Gr...   \n",
            "1  Are there sexual predator watchdog groups in S...   \n",
            "2  Camping in Sooke Hi, we’re planning on camping...   \n",
            "3  Cockatoo escaped across from Saseenos. Please ...   \n",
            "4  Help: What’s the best option for housing right...   \n",
            "\n",
            "                                            Document  \n",
            "0  Are power outages in Sooke a regular thing? Gr...  \n",
            "1  Are there sexual predator watchdog groups in S...  \n",
            "2  Camping in Sooke Hi, we’re planning on camping...  \n",
            "3  Cockatoo escaped across from Saseenos. Please ...  \n",
            "4  Help: What’s the best option for housing right...  \n",
            "Data fetched and data/saved to reddit_documents_df.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d19ec9f-12fb-4e74-a0d0-574a938ccd12\", \"reddit_documents_df.csv\", 46414032)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpNFN6KB-xdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "documents_df.to_csv('/content/SWB-GVCEH/data/reddit_documents_df.csv', index=False)\n",
        "print(\"Data fetched and data/saved to reddit_documents_df.csv\")\n",
        "# Path to the CSV file\n",
        "file_path = '/content/SWB-GVCEH/data/reddit_documents_df.csv'\n",
        "# Trigger a download to your local machine\n",
        "files.download(file_path)"
      ],
      "metadata": {
        "id": "qBSZ-92Z-nFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your git remote URL to include the PAT for authentication\n",
        "repo_url = 'https://github.com/alex-jk/SWB-GVCEH.git'  # Replace with your repository's URL\n",
        "pat = os.environ['GITHUB_PAT']\n",
        "repo_url_with_token = repo_url[:8] + pat + \"@\" + repo_url[8:]\n",
        "\n",
        "!git remote set-url origin {repo_url_with_token}\n",
        "\n",
        "# Navigate to the repository directory, add, commit, and push the new CSV file\n",
        "%cd /content/SWB-GVCEH\n",
        "!git add 'data/reddit_documents_df.csv'\n",
        "!git commit -m \"Add fetched reddit_documents_df CSV\"\n",
        "!git push origin main  # Replace 'main' with your branch name if it's different\n",
        "\n",
        "# Reset the remote URL to the original without the PAT\n",
        "!git remote set-url origin {repo_url}\n",
        "print(\"CSV file pushed to GitHub.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plLLEvM39SCM",
        "outputId": "a825594b-e48a-4273-f25d-1e9cc5b64b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SWB-GVCEH\n",
            "[main baf7738] Add fetched reddit_documents_df CSV\n",
            " 1 file changed, 270673 insertions(+)\n",
            " create mode 100644 data/reddit_documents_df.csv\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 8.77 MiB | 2.85 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/alex-jk/SWB-GVCEH.git\n",
            "   1ab4f04..baf7738  main -> main\n",
            "CSV file pushed to GitHub.\n"
          ]
        }
      ]
    }
  ]
}